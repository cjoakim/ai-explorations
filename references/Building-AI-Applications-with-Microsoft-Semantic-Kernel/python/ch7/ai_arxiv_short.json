{"id":"2101.00010","authors":"Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, Adina Williams","title":"UnNatural Language Inference","abstract":"  Recent investigations into the inner-workings of state-of-the-art large-scale\npre-trained Transformer-based Natural Language Understanding (NLU) models\nindicate that they appear to know humanlike syntax, at least to some extent. We\nprovide novel evidence that complicates this claim: we find that\nstate-of-the-art Natural Language Inference (NLI) models assign the same labels\nto permuted examples as they do to the original, i.e. they are largely\ninvariant to random word-order permutations. This behavior notably differs from\nthat of humans; we struggle with ungrammatical sentences. To measure the\nseverity of this issue, we propose a suite of metrics and investigate which\nproperties of particular permutations lead models to be word-order invariant.\nIn the MNLI dataset, for example, we find almost all (98.7%) examples contain\nat least one permutation which elicits the gold label. Models are sometimes\neven able to assign gold labels to permutations that they originally failed to\npredict correctly. We provide a comprehensive empirical evaluation of this\nphenomenon, and further show that this issue exists for both Transformers and\npre-Transformer RNN \/ ConvNet based encoders, as well as across multiple\nlanguages (English and Mandarin Chinese). Our code and data are available at\nhttps:\/\/github.com\/facebookresearch\/unlu.\n"}
{"id":"2101.00027","authors":"Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe,\n  Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn\n  Presser, Connor Leahy","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling","abstract":"  Recent work has demonstrated that increased training dataset diversity\nimproves general cross-domain knowledge and downstream generalization\ncapability for large-scale language models. With this in mind, we present\n\\textit{the Pile}: an 825 GiB English text corpus targeted at training\nlarge-scale language models. The Pile is constructed from 22 diverse\nhigh-quality subsets -- both existing and newly constructed -- many of which\nderive from academic or professional sources. Our evaluation of the untuned\nperformance of GPT-2 and GPT-3 on the Pile shows that these models struggle on\nmany of its components, such as academic writing. Conversely, models trained on\nthe Pile improve significantly over both Raw CC and CC-100 on all components of\nthe Pile, while improving performance on downstream evaluations. Through an\nin-depth exploratory analysis, we document potentially concerning aspects of\nthe data for prospective users. We make publicly available the code used in its\nconstruction.\n"}
{"id":"2101.00036","authors":"Yuta Nakamura (1), Shouhei Hanaoka (1 and 2), Yukihiro Nomura (3 and\n  4), Naoto Hayashi (3), Osamu Abe (1 and 2), Shuntaro Yada (5), Shoko Wakamiya\n  (5), Eiji Aramaki (5) ((1) The University of Tokyo, (2) The Department of\n  Radiology, The University of Tokyo Hospital, (3) The Department of\n  Computational Diagnostic Radiology and Preventive Medicine, The University of\n  Tokyo Hospital, (4) Chiba University, (5) Nara Institute of Science and\n  Technology)","title":"KART: Parameterization of Privacy Leakage Scenarios from Pre-trained\n  Language Models","abstract":"  For the safe sharing pre-trained language models, no guidelines exist at\npresent owing to the difficulty in estimating the upper bound of the risk of\nprivacy leakage. One problem is that previous studies have assessed the risk\nfor different real-world privacy leakage scenarios and attack methods, which\nreduces the portability of the findings. To tackle this problem, we represent\ncomplex real-world privacy leakage scenarios under a universal\nparameterization, \\textit{Knowledge, Anonymization, Resource, and Target}\n(KART). KART parameterization has two merits: (i) it clarifies the definition\nof privacy leakage in each experiment and (ii) it improves the comparability of\nthe findings of risk assessments. We show that previous studies can be simply\nreviewed by parameterizing the scenarios with KART. We also demonstrate privacy\nrisk assessments in different scenarios under the same attack method, which\nsuggests that KART helps approximate the upper bound of risk under a specific\nattack or scenario. We believe that KART helps integrate past and future\nfindings on privacy risk and will contribute to a standard for sharing language\nmodels.\n"}
{"id":"2101.00056","authors":"Rajaswa Patil, Yaman Kumar Singla, Rajiv Ratn Shah, Mika Hama and\n  Roger Zimmermann","title":"Towards Modelling Coherence in Spoken Discourse","abstract":"  While there has been significant progress towards modelling coherence in\nwritten discourse, the work in modelling spoken discourse coherence has been\nquite limited. Unlike the coherence in text, coherence in spoken discourse is\nalso dependent on the prosodic and acoustic patterns in speech. In this paper,\nwe model coherence in spoken discourse with audio-based coherence models. We\nperform experiments with four coherence-related tasks with spoken discourses.\nIn our experiments, we evaluate machine-generated speech against the speech\ndelivered by expert human speakers. We also compare the spoken discourses\ngenerated by human language learners of varying language proficiency levels.\nOur results show that incorporating the audio modality along with the text\nbenefits the coherence models in performing downstream coherence related tasks\nwith spoken discourses.\n"}
{"id":"2101.00063","authors":"Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang,\n  Jingjing Liu","title":"EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets","abstract":"  Heavily overparameterized language models such as BERT, XLNet and T5 have\nachieved impressive success in many NLP tasks. However, their high model\ncomplexity requires enormous computation resources and extremely long training\ntime for both pre-training and fine-tuning. Many works have studied model\ncompression on large NLP models, but only focusing on reducing inference time\nwhile still requiring an expensive training process. Other works use extremely\nlarge batch sizes to shorten the pre-training time, at the expense of higher\ncomputational resource demands. In this paper, inspired by the Early-Bird\nLottery Tickets recently studied for computer vision tasks, we propose\nEarlyBERT, a general computationally-efficient training algorithm applicable to\nboth pre-training and fine-tuning of large-scale language models. By slimming\nthe self-attention and fully-connected sub-layers inside a transformer, we are\nthe first to identify structured winning tickets in the early stage of BERT\ntraining. We apply those tickets towards efficient BERT training, and conduct\ncomprehensive pre-training and fine-tuning experiments on GLUE and SQuAD\ndownstream tasks. Our results show that EarlyBERT achieves comparable\nperformance to standard BERT, with 35~45% less training time. Code is available\nat https:\/\/github.com\/VITA-Group\/EarlyBERT.\n"}
{"id":"2101.00078","authors":"Anjalie Field, Chan Young Park, Kevin Z. Lin, Yulia Tsvetkov","title":"Controlled Analyses of Social Biases in Wikipedia Bios","abstract":"  Social biases on Wikipedia, a widely-read global platform, could greatly\ninfluence public opinion. While prior research has examined man\/woman gender\nbias in biography articles, possible influences of other demographic attributes\nlimit conclusions. In this work, we present a methodology for analyzing\nWikipedia pages about people that isolates dimensions of interest (e.g.,\ngender), from other attributes (e.g., occupation). Given a target corpus for\nanalysis (e.g.~biographies about women), we present a method for constructing a\ncomparison corpus that matches the target corpus in as many attributes as\npossible, except the target one. We develop evaluation metrics to measure how\nwell the comparison corpus aligns with the target corpus and then examine how\narticles about gender and racial minorities (cis. women, non-binary people,\ntransgender women, and transgender men; African American, Asian American, and\nHispanic\/Latinx American people) differ from other articles. In addition to\nidentifying suspect social biases, our results show that failing to control for\ncovariates can result in different conclusions and veil biases. Our\ncontributions include methodology that facilitates further analyses of bias in\nWikipedia articles, findings that can aid Wikipedia editors in reducing biases,\nand a framework and evaluation metrics to guide future work in this area.\n"}
{"id":"2101.00117","authors":"Jean Maillard, Vladimir Karpukhin, Fabio Petroni, Wen-tau Yih, Barlas\n  O\\u{g}uz, Veselin Stoyanov, Gargi Ghosh","title":"Multi-task Retrieval for Knowledge-Intensive Tasks","abstract":"  Retrieving relevant contexts from a large corpus is a crucial step for tasks\nsuch as open-domain question answering and fact checking. Although neural\nretrieval outperforms traditional methods like tf-idf and BM25, its performance\ndegrades considerably when applied to out-of-domain data.\n  Driven by the question of whether a neural retrieval model can be universal\nand perform robustly on a wide variety of problems, we propose a multi-task\ntrained model. Our approach not only outperforms previous methods in the\nfew-shot setting, but also rivals specialised neural retrievers, even when\nin-domain training data is abundant. With the help of our retriever, we improve\nexisting models for downstream tasks and closely match or improve the state of\nthe art on multiple benchmarks.\n"}
{"id":"2101.00121","authors":"Karen Hambardzumyan, Hrant Khachatrian, Jonathan May","title":"WARP: Word-level Adversarial ReProgramming","abstract":"  Transfer learning from pretrained language models recently became the\ndominant approach for solving many NLP tasks. A common approach to transfer\nlearning for multiple tasks that maximize parameter sharing trains one or more\ntask-specific layers on top of the language model. In this paper, we present an\nalternative approach based on adversarial reprogramming, which extends earlier\nwork on automatic prompt generation. Adversarial reprogramming attempts to\nlearn task-specific word embeddings that, when concatenated to the input text,\ninstruct the language model to solve the specified task. Using up to 25K\ntrainable parameters per task, this approach outperforms all existing methods\nwith up to 25M trainable parameters on the public leaderboard of the GLUE\nbenchmark. Our method, initialized with task-specific human-readable prompts,\nalso works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks\nwith just 32 training samples.\n"}
{"id":"2101.00123","authors":"Wasi Uddin Ahmad, Jianfeng Chi, Tu Le, Thomas Norton, Yuan Tian,\n  Kai-Wei Chang","title":"Intent Classification and Slot Filling for Privacy Policies","abstract":"  Understanding privacy policies is crucial for users as it empowers them to\nlearn about the information that matters to them. Sentences written in a\nprivacy policy document explain privacy practices, and the constituent text\nspans convey further specific information about that practice. We refer to\npredicting the privacy practice explained in a sentence as intent\nclassification and identifying the text spans sharing specific information as\nslot filling. In this work, we propose PolicyIE, an English corpus consisting\nof 5,250 intent and 11,788 slot annotations spanning 31 privacy policies of\nwebsites and mobile applications. PolicyIE corpus is a challenging real-world\nbenchmark with limited labeled examples reflecting the cost of collecting\nlarge-scale annotations from domain experts. We present two alternative neural\napproaches as baselines, (1) intent classification and slot filling as a joint\nsequence tagging and (2) modeling them as a sequence-to-sequence (Seq2Seq)\nlearning task. The experiment results show that both approaches perform\ncomparably in intent classification, while the Seq2Seq method outperforms the\nsequence tagging approach in slot filling by a large margin. We perform a\ndetailed error analysis to reveal the challenges of the proposed corpus.\n"}
{"id":"2101.00124","authors":"I-Hung Hsu, Xiao Guo, Premkumar Natarajan, Nanyun Peng","title":"Discourse-level Relation Extraction via Graph Pooling","abstract":"  The ability to capture complex linguistic structures and long-term\ndependencies among words in the passage is essential for discourse-level\nrelation extraction (DRE) tasks. Graph neural networks (GNNs), one of the\nmethods to encode dependency graphs, have been shown effective in prior works\nfor DRE. However, relatively little attention has been paid to receptive fields\nof GNNs, which can be crucial for cases with extremely long text that requires\ndiscourse understanding. In this work, we leverage the idea of graph pooling\nand propose to use pooling-unpooling framework on DRE tasks. The pooling branch\nreduces the graph size and enables the GNNs to obtain larger receptive fields\nwithin fewer layers; the unpooling branch restores the pooled graph to its\noriginal resolution so that representations for entity mention can be\nextracted. We propose Clause Matching (CM), a novel linguistically inspired\ngraph pooling method for NLP tasks. Experiments on two DRE datasets demonstrate\nthat our models significantly improve over baselines when modeling long-term\ndependencies is required, which shows the effectiveness of the\npooling-unpooling framework and our CM pooling method.\n"}
{"id":"2101.00130","authors":"Jiaman Wu, Dezhi Hong, Rajesh Gupta, Jingbo Shang","title":"Sensei: Self-Supervised Sensor Name Segmentation","abstract":"  A sensor name, typically an alphanumeric string, encodes the key context\n(e.g., function and location) of a sensor needed for deploying smart building\napplications. Sensor names, however, are curated in a building vendor-specific\nmanner using different structures and vocabularies that are often esoteric.\nThey thus require tremendous manual effort to annotate on a per-building basis;\neven to just segment these sensor names into meaningful chunks. In this paper,\nwe propose a fully automated self-supervised framework, Sensei, which can learn\nto segment sensor names without any human annotation. Specifically, we employ a\nneural language model to capture the underlying sensor naming structure and\nthen induce self-supervision based on information from the language model to\nbuild the segmentation model. Extensive experiments on five real-world\nbuildings comprising thousands of sensors demonstrate the superiority of Sensei\nover baseline methods.\n"}
{"id":"2101.00133","authors":"Sewon Min, Jordan Boyd-Graber, Chris Alberti, Danqi Chen, Eunsol Choi,\n  Michael Collins, Kelvin Guu, Hannaneh Hajishirzi, Kenton Lee, Jennimaria\n  Palomaki, Colin Raffel, Adam Roberts, Tom Kwiatkowski, Patrick Lewis, Yuxiang\n  Wu, Heinrich K\\\"uttler, Linqing Liu, Pasquale Minervini, Pontus Stenetorp,\n  Sebastian Riedel, Sohee Yang, Minjoon Seo, Gautier Izacard, Fabio Petroni,\n  Lucas Hosseini, Nicola De Cao, Edouard Grave, Ikuya Yamada, Sonse Shimaoka,\n  Masatoshi Suzuki, Shumpei Miyawaki, Shun Sato, Ryo Takahashi, Jun Suzuki,\n  Martin Fajcik, Martin Docekal, Karel Ondrej, Pavel Smrz, Hao Cheng, Yelong\n  Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen, Jianfeng Gao, Barlas Oguz,\n  Xilun Chen, Vladimir Karpukhin, Stan Peshterliev, Dmytro Okhonko, Michael\n  Schlichtkrull, Sonal Gupta, Yashar Mehdad, Wen-tau Yih","title":"NeurIPS 2020 EfficientQA Competition: Systems, Analyses and Lessons\n  Learned","abstract":"  We review the EfficientQA competition from NeurIPS 2020. The competition\nfocused on open-domain question answering (QA), where systems take natural\nlanguage questions as input and return natural language answers. The aim of the\ncompetition was to build systems that can predict correct answers while also\nsatisfying strict on-disk memory budgets. These memory budgets were designed to\nencourage contestants to explore the trade-off between storing retrieval\ncorpora or the parameters of learned models. In this report, we describe the\nmotivation and organization of the competition, review the best submissions,\nand analyze system predictions to inform a discussion of evaluation for\nopen-domain QA.\n"}
{"id":"2101.00146","authors":"Leibo Liu, Oscar Perez-Concha, Anthony Nguyen, Vicki Bennett, Louisa\n  Jorm","title":"De-identifying Australian Hospital Discharge Summaries: An End-to-End\n  Framework using Ensemble of Deep Learning Models","abstract":"  Electronic Medical Records (EMRs) contain clinical narrative text that is of\ngreat potential value to medical researchers. However, this information is\nmixed with Personally Identifiable Information (PII) that presents risks to\npatient and clinician confidentiality. This paper presents an end-to-end\ndeidentification framework to automatically remove PII from Australian hospital\ndischarge summaries. Our corpus included 600 hospital discharge summaries which\nwere extracted from the EMRs of two principal referral hospitals in Sydney,\nAustralia. Our end-to-end de-identification framework consists of three\ncomponents: 1) Annotation: labelling of PII in the 600 hospital discharge\nsummaries using five pre-defined categories: person, address, date of birth,\nindividual identification number, phone\/fax number; 2) Modelling: training six\nnamed entity recognition (NER) deep learning base-models on balanced and\nimbalanced datasets; and evaluating ensembles that combine all six base-models,\nthe three base-models with the best F1 scores and the three base-models with\nthe best recall scores respectively, using token-level majority voting and\nstacking methods; and 3) De-identification: removing PII from the hospital\ndischarge summaries. Our results showed that the ensemble model combined using\nthe stacking Support Vector Machine (SVM) method on the three base-models with\nthe best F1 scores achieved excellent results with a F1 score of 99.16% on the\ntest set of our corpus. We also evaluated the robustness of our modelling\ncomponent on the 2014 i2b2 de-identification dataset. Our ensemble model, which\nuses the token-level majority voting method on all six basemodels, achieved the\nhighest F1 score of 96.24% at strict entity matching and the highest F1 score\nof 98.64% at binary token-level matching compared to two state-of-the-art\nmethods.\n"}
{"id":"2101.00148","authors":"Haoyue Shi, Luke Zettlemoyer, Sida I. Wang","title":"Bilingual Lexicon Induction via Unsupervised Bitext Construction and\n  Word Alignment","abstract":"  Bilingual lexicons map words in one language to their translations in\nanother, and are typically induced by learning linear projections to align\nmonolingual word embedding spaces. In this paper, we show it is possible to\nproduce much higher quality lexicons with methods that combine (1) unsupervised\nbitext mining and (2) unsupervised word alignment. Directly applying a pipeline\nthat uses recent algorithms for both subproblems significantly improves induced\nlexicon quality and further gains are possible by learning to filter the\nresulting lexical entries, with both unsupervised and semi-supervised schemes.\nOur final model outperforms the state of the art on the BUCC 2020 shared task\nby 14 $F_1$ points averaged over 12 language pairs, while also providing a more\ninterpretable approach that allows for rich reasoning of word meaning in\ncontext. Further analysis of our output and the standard reference lexicons\nsuggests they are of comparable quality, and new benchmarks may be needed to\nmeasure further progress on this task.\n"}
{"id":"2101.00151","authors":"Hung Le and Chinnadhurai Sankar and Seungwhan Moon and Ahmad Beirami\n  and Alborz Geramifard and Satwik Kottur","title":"DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded\n  Dialogue","abstract":"  A video-grounded dialogue system is required to understand both dialogue,\nwhich contains semantic dependencies from turn to turn, and video, which\ncontains visual cues of spatial and temporal scene variations. Building such\ndialogue systems is a challenging problem, involving various reasoning types on\nboth visual and language inputs. Existing benchmarks do not have enough\nannotations to thoroughly analyze dialogue systems and understand their\ncapabilities and limitations in isolation. These benchmarks are also not\nexplicitly designed to minimise biases that models can exploit without actual\nreasoning. To address these limitations, in this paper, we present DVD, a\nDiagnostic Dataset for Video-grounded Dialogues. The dataset is designed to\ncontain minimal biases and has detailed annotations for the different types of\nreasoning over the spatio-temporal space of video. Dialogues are synthesized\nover multiple question turns, each of which is injected with a set of\ncross-turn semantic relationships. We use DVD to analyze existing approaches,\nproviding interesting insights into their abilities and limitations. In total,\nDVD is built from $11k$ CATER synthetic videos and contains $10$ instances of\n$10$-round dialogues for each video, resulting in more than $100k$ dialogues\nand $1M$ question-answer pairs. Our code and dataset are publicly available at\nhttps:\/\/github.com\/facebookresearch\/DVDialogues.\n"}
{"id":"2101.00153","authors":"Liu Bin, Yin Guosheng","title":"Graphmax for Text Generation","abstract":"  In text generation, a large language model (LM) makes a choice of each new\nword based only on the former selection of its context using the softmax\nfunction. Nevertheless, the link statistics information of concurrent words\nbased on a scene-specific corpus is valuable in choosing the next word, which\ncan help to ensure the topic of the generated text to be aligned with the\ncurrent task. To fully explore the co-occurrence information,we propose a\ngraphmax function for task-specific text generation. Using the graph-based\nregularization, graphmax enables the final word choice to be determined by both\nthe global knowledge from the LM and the local knowledge from the\nscene-specific corpus. The traditional softmax function is regularized with a\ngraph total variation (GTV) term, which incorporates the local knowledge into\nthe LM and encourages the model to consider the statistical relationships\nbetween words in a scene-specific corpus. The proposed graphmax is versatile\nand can be readily plugged into any large pre-trained LM for text generation\nand machine translation. Through extensive experiments, we demonstrate that the\nnew GTV-based regularization can improve performances in various natural\nlanguage processing tasks in comparison with existing methods. Moreover,\nthrough human experiments, we observe that participants can easily distinguish\nthe text generated by graphmax or softmax.\n"}
{"id":"2101.00154","authors":"Tianqing Fang, Hongming Zhang, Weiqi Wang, Yangqiu Song, Bin He","title":"DISCOS: Bridging the Gap between Discourse Knowledge and Commonsense\n  Knowledge","abstract":"  Commonsense knowledge is crucial for artificial intelligence systems to\nunderstand natural language. Previous commonsense knowledge acquisition\napproaches typically rely on human annotations (for example, ATOMIC) or text\ngeneration models (for example, COMET.) Human annotation could provide\nhigh-quality commonsense knowledge, yet its high cost often results in\nrelatively small scale and low coverage. On the other hand, generation models\nhave the potential to automatically generate more knowledge. Nonetheless,\nmachine learning models often fit the training data well and thus struggle to\ngenerate high-quality novel knowledge. To address the limitations of previous\napproaches, in this paper, we propose an alternative commonsense knowledge\nacquisition framework DISCOS (from DIScourse to COmmonSense), which\nautomatically populates expensive complex commonsense knowledge to more\naffordable linguistic knowledge resources. Experiments demonstrate that we can\nsuccessfully convert discourse knowledge about eventualities from ASER, a\nlarge-scale discourse knowledge graph, into if-then commonsense knowledge\ndefined in ATOMIC without any additional annotation effort. Further study\nsuggests that DISCOS significantly outperforms previous supervised approaches\nin terms of novelty and diversity with comparable quality. In total, we can\nacquire 3.4M ATOMIC-like inferential commonsense knowledge by populating ATOMIC\non the core part of ASER. Codes and data are available at\nhttps:\/\/github.com\/HKUST-KnowComp\/DISCOS-commonsense.\n"}
{"id":"2101.00160","authors":"Hyunjae Kim, Jaewoo Kang","title":"How Do Your Biomedical Named Entity Recognition Models Generalize to\n  Novel Entities?","abstract":"  The number of biomedical literature on new biomedical concepts is rapidly\nincreasing, which necessitates a reliable biomedical named entity recognition\n(BioNER) model for identifying new and unseen entity mentions. However, it is\nquestionable whether existing models can effectively handle them. In this work,\nwe systematically analyze the three types of recognition abilities of BioNER\nmodels: memorization, synonym generalization, and concept generalization. We\nfind that although current best models achieve state-of-the-art performance on\nbenchmarks based on overall performance, they have limitations in identifying\nsynonyms and new biomedical concepts, indicating they are overestimated in\nterms of their generalization abilities. We also investigate failure cases of\nmodels and identify several difficulties in recognizing unseen mentions in\nbiomedical literature as follows: (1) models tend to exploit dataset biases,\nwhich hinders the models' abilities to generalize, and (2) several biomedical\nnames have novel morphological patterns with weak name regularity, and models\nfail to recognize them. We apply a statistics-based debiasing method to our\nproblem as a simple remedy and show the improvement in generalization to unseen\nmentions. We hope that our analyses and findings would be able to facilitate\nfurther research into the generalization capabilities of NER models in a domain\nwhere their reliability is of utmost importance.\n"}
{"id":"2101.00167","authors":"Yi Cheng, Sujian Li, Yueyuan Li","title":"Unifying Discourse Resources with Dependency Framework","abstract":"  For text-level discourse analysis, there are various discourse schemes but\nrelatively few labeled data, because discourse research is still immature and\nit is labor-intensive to annotate the inner logic of a text. In this paper, we\nattempt to unify multiple Chinese discourse corpora under different annotation\nschemes with discourse dependency framework by designing semi-automatic methods\nto convert them into dependency structures. We also implement several benchmark\ndependency parsers and research on how they can leverage the unified data to\nimprove performance.\n"}
{"id":"2101.00173","authors":"Mohamed Elhoseiny, Kai Yi, Mohamed Elfeki","title":"CIZSL++: Creativity Inspired Generative Zero-Shot Learning","abstract":"  Zero-shot learning (ZSL) aims at understanding unseen categories with no\ntraining examples from class-level descriptions. To improve the discriminative\npower of ZSL, we model the visual learning process of unseen categories with\ninspiration from the psychology of human creativity for producing novel art.\nFirst, we propose CIZSL-v1 as a creativity inspired model for generative ZSL.\nWe relate ZSL to human creativity by observing that ZSL is about recognizing\nthe unseen, and creativity is about creating a likable unseen. We introduce a\nlearning signal inspired by creativity literature that explores the unseen\nspace with hallucinated class-descriptions and encourages careful deviation of\ntheir visual feature generations from seen classes while allowing knowledge\ntransfer from seen to unseen classes. Second, CIZSL-v2 is proposed as an\nimproved version of CIZSL-v1 for generative zero-shot learning. CIZSL-v2\nconsists of an investigation of additional inductive losses for unseen classes\nalong with a semantic guided discriminator. Empirically, we show consistently\nthat CIZSL losses can improve generative ZSL models on the challenging task of\ngeneralized ZSL from a noisy text on CUB and NABirds datasets. We also show the\nadvantage of our approach to Attribute-based ZSL on AwA2, aPY, and SUN\ndatasets. We also show that CIZSL-v2 has improved performance compared to\nCIZSL-v1.\n"}
{"id":"2101.00178","authors":"Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen,\n  Jianfeng Gao","title":"UnitedQA: A Hybrid Approach for Open Domain Question Answering","abstract":"  To date, most of recent work under the retrieval-reader framework for\nopen-domain QA focuses on either extractive or generative reader exclusively.\nIn this paper, we study a hybrid approach for leveraging the strengths of both\nmodels. We apply novel techniques to enhance both extractive and generative\nreaders built upon recent pretrained neural language models, and find that\nproper training methods can provide large improvement over previous\nstate-of-the-art models. We demonstrate that a simple hybrid approach by\ncombining answers from both readers can efficiently take advantages of\nextractive and generative answer inference strategies and outperforms single\nmodels as well as homogeneous ensembles. Our approach outperforms previous\nstate-of-the-art models by 3.3 and 2.7 points in exact match on\nNaturalQuestions and TriviaQA respectively.\n"}
{"id":"2101.00180","authors":"Sunil Gundapu, Radhika Mamidi","title":"Transformer based Automatic COVID-19 Fake News Detection System","abstract":"  Recent rapid technological advancements in online social networks such as\nTwitter have led to a great incline in spreading false information and fake\nnews. Misinformation is especially prevalent in the ongoing coronavirus disease\n(COVID-19) pandemic, leading to individuals accepting bogus and potentially\ndeleterious claims and articles. Quick detection of fake news can reduce the\nspread of panic and confusion among the public. For our analysis in this paper,\nwe report a methodology to analyze the reliability of information shared on\nsocial media pertaining to the COVID-19 pandemic. Our best approach is based on\nan ensemble of three transformer models (BERT, ALBERT, and XLNET) to detecting\nfake news. This model was trained and evaluated in the context of the\nConstraintAI 2021 shared task COVID19 Fake News Detection in English. Our\nsystem obtained 0.9855 f1-score on testset and ranked 5th among 160 teams.\n"}
{"id":"2101.00190","authors":"Xiang Lisa Li and Percy Liang","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation","abstract":"  Fine-tuning is the de facto way to leverage large pretrained language models\nto perform downstream tasks. However, it modifies all the language model\nparameters and therefore necessitates storing a full copy for each task. In\nthis paper, we propose prefix-tuning, a lightweight alternative to fine-tuning\nfor natural language generation tasks, which keeps language model parameters\nfrozen, but optimizes a small continuous task-specific vector (called the\nprefix). Prefix-tuning draws inspiration from prompting, allowing subsequent\ntokens to attend to this prefix as if it were \"virtual tokens\". We apply\nprefix-tuning to GPT-2 for table-to-text generation and to BART for\nsummarization. We find that by learning only 0.1\\% of the parameters,\nprefix-tuning obtains comparable performance in the full data setting,\noutperforms fine-tuning in low-data settings, and extrapolates better to\nexamples with topics unseen during training.\n"}
{"id":"2101.00196","authors":"Zhengxuan Wu, Desmond C. Ong","title":"On Explaining Your Explanations of BERT: An Empirical Study with\n  Sequence Classification","abstract":"  BERT, as one of the pretrianed language models, attracts the most attention\nin recent years for creating new benchmarks across GLUE tasks via fine-tuning.\nOne pressing issue is to open up the blackbox and explain the decision makings\nof BERT. A number of attribution techniques have been proposed to explain BERT\nmodels, but are often limited to sequence to sequence tasks. In this paper, we\nadapt existing attribution methods on explaining decision makings of BERT in\nsequence classification tasks. We conduct extensive analyses of four existing\nattribution methods by applying them to four different datasets in sentiment\nanalysis. We compare the reliability and robustness of each method via various\nablation studies. Furthermore, we test whether attribution methods explain\ngeneralized semantics across semantically similar tasks. Our work provides\nsolid guidance for using attribution methods to explain decision makings of\nBERT for downstream classification tasks.\n"}
{"id":"2101.00204","authors":"Abhik Bhattacharjee, Tahmid Hasan, Wasi Uddin Ahmad, Kazi Samin, Md\n  Saiful Islam, Anindya Iqbal, M. Sohel Rahman, Rifat Shahriyar","title":"BanglaBERT: Language Model Pretraining and Benchmarks for Low-Resource\n  Language Understanding Evaluation in Bangla","abstract":"  In this work, we introduce BanglaBERT, a BERT-based Natural Language\nUnderstanding (NLU) model pretrained in Bangla, a widely spoken yet\nlow-resource language in the NLP literature. To pretrain BanglaBERT, we collect\n27.5 GB of Bangla pretraining data (dubbed `Bangla2B+') by crawling 110 popular\nBangla sites. We introduce two downstream task datasets on natural language\ninference and question answering and benchmark on four diverse NLU tasks\ncovering text classification, sequence labeling, and span prediction. In the\nprocess, we bring them under the first-ever Bangla Language Understanding\nBenchmark (BLUB). BanglaBERT achieves state-of-the-art results outperforming\nmultilingual and monolingual models. We are making the models, datasets, and a\nleaderboard publicly available at https:\/\/github.com\/csebuetnlp\/banglabert to\nadvance Bangla NLP.\n"}
{"id":"2101.00234","authors":"Machel Reid, Edison Marrese-Taylor and Yutaka Matsuo","title":"Subformer: Exploring Weight Sharing for Parameter Efficiency in\n  Generative Transformers","abstract":"  Transformers have shown improved performance when compared to previous\narchitectures for sequence processing such as RNNs. Despite their sizeable\nperformance gains, as recently suggested, the model is computationally\nexpensive to train and with a high parameter budget. In light of this, we\nexplore parameter-sharing methods in Transformers with a specific focus on\ngenerative models. We perform an analysis of different parameter\nsharing\/reduction methods and develop the Subformer. Our model combines\nsandwich-style parameter sharing, which overcomes naive cross-layer parameter\nsharing in generative models, and self-attentive embedding factorization\n(SAFE). Experiments on machine translation, abstractive summarization and\nlanguage modeling show that the Subformer can outperform the Transformer even\nwhen using significantly fewer parameters.\n"}
{"id":"2101.00259","authors":"Sajad Norouzi, Keyi Tang, Yanshuai Cao","title":"Code Generation from Natural Language with Less Prior and More\n  Monolingual Data","abstract":"  Training datasets for semantic parsing are typically small due to the higher\nexpertise required for annotation than most other NLP tasks. As a result,\nmodels for this application usually need additional prior knowledge to be built\ninto the architecture or algorithm. The increased dependency on human experts\nhinders automation and raises the development and maintenance costs in\npractice. This work investigates whether a generic transformer-based seq2seq\nmodel can achieve competitive performance with minimal code-generation-specific\ninductive bias design. By exploiting a relatively sizeable monolingual corpus\nof the target programming language, which is cheap to mine from the web, we\nachieved 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa.\nBoth are SOTA to the best of our knowledge. This positive evidence highlights a\npotentially easier path toward building accurate semantic parsers in practice.\n"}
{"id":"2101.00265","authors":"Xiaopeng Lu, Tiancheng Zhao, Kyusong Lee","title":"VisualSparta: An Embarrassingly Simple Approach to Large-scale\n  Text-to-Image Search with Weighted Bag-of-words","abstract":"  Text-to-image retrieval is an essential task in cross-modal information\nretrieval, i.e., retrieving relevant images from a large and unlabelled dataset\ngiven textual queries. In this paper, we propose VisualSparta, a novel\n(Visual-text Sparse Transformer Matching) model that shows significant\nimprovement in terms of both accuracy and efficiency. VisualSparta is capable\nof outperforming previous state-of-the-art scalable methods in MSCOCO and\nFlickr30K. We also show that it achieves substantial retrieving speed\nadvantages, i.e., for a 1 million image index, VisualSparta using CPU gets\n~391X speedup compared to CPU vector search and ~5.4X speedup compared to\nvector search with GPU acceleration. Experiments show that this speed advantage\neven gets bigger for larger datasets because VisualSparta can be efficiently\nimplemented as an inverted index. To the best of our knowledge, VisualSparta is\nthe first transformer-based text-to-image retrieval model that can achieve\nreal-time searching for large-scale datasets, with significant accuracy\nimprovement compared to previous state-of-the-art methods.\n"}
{"id":"2101.00288","authors":"Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, Daniel S. Weld","title":"Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and\n  Improving Models","abstract":"  While counterfactual examples are useful for analysis and training of NLP\nmodels, current generation methods either rely on manual labor to create very\nfew counterfactuals, or only instantiate limited types of perturbations such as\nparaphrases or word substitutions. We present Polyjuice, a general-purpose\ncounterfactual generator that allows for control over perturbation types and\nlocations, trained by finetuning GPT-2 on multiple datasets of paired\nsentences. We show that Polyjuice produces diverse sets of realistic\ncounterfactuals, which in turn are useful in various distinct applications:\nimproving training and evaluation on three different tasks (with around 70%\nless annotation effort than manual generation), augmenting state-of-the-art\nexplanation techniques, and supporting systematic counterfactual error analysis\nby revealing behaviors easily missed by human experts.\n"}
{"id":"2101.00294","authors":"Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao,\n  Jiawei Han, Weizhu Chen","title":"Rider: Reader-Guided Passage Reranking for Open-Domain Question\n  Answering","abstract":"  Current open-domain question answering systems often follow a\nRetriever-Reader architecture, where the retriever first retrieves relevant\npassages and the reader then reads the retrieved passages to form an answer. In\nthis paper, we propose a simple and effective passage reranking method, named\nReader-guIDEd Reranker (RIDER), which does not involve training and reranks the\nretrieved passages solely based on the top predictions of the reader before\nreranking. We show that RIDER, despite its simplicity, achieves 10 to 20\nabsolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains\nwithout refining the retriever or reader. In addition, RIDER, without any\ntraining, outperforms state-of-the-art transformer-based supervised rerankers.\nRemarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM\non the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are\nused as the reader input after passage reranking.\n"}
{"id":"2101.00297","authors":"Jeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, Antoine Bosselut","title":"Analyzing Commonsense Emergence in Few-shot Knowledge Models","abstract":"  Recently, commonsense knowledge models - pretrained language models (LM)\nfine-tuned on knowledge graph (KG) tuples - showed that considerable amounts of\ncommonsense knowledge can be encoded in the parameters of large language\nmodels. However, as parallel studies show that LMs are poor hypothesizers of\ndeclarative commonsense relationships on their own, it remains unclear whether\nthis knowledge is learned during pretraining or from fine-tuning on KG\nexamples. To investigate this question, we train commonsense knowledge models\nin few-shot settings to study the emergence of their commonsense representation\nabilities. Our results show that commonsense knowledge models can rapidly adapt\nfrom limited examples, indicating that KG fine-tuning serves to learn an\ninterface to encoded knowledge learned during pretraining. Importantly, our\nanalysis of absolute, angular, and distributional parameter changes during\nfew-shot fine-tuning provides novel insights into how this interface is\nlearned.\n"}
{"id":"2101.00345","authors":"Yasumasa Onoe, Michael Boratko, Andrew McCallum, Greg Durrett","title":"Modeling Fine-Grained Entity Types with Box Embeddings","abstract":"  Neural entity typing models typically represent fine-grained entity types as\nvectors in a high-dimensional space, but such spaces are not well-suited to\nmodeling these types' complex interdependencies. We study the ability of box\nembeddings, which embed concepts as d-dimensional hyperrectangles, to capture\nhierarchies of types even when these relationships are not defined explicitly\nin the ontology. Our model represents both types and entity mentions as boxes.\nEach mention and its context are fed into a BERT-based model to embed that\nmention in our box space; essentially, this model leverages typological clues\npresent in the surface text to hypothesize a type representation for the\nmention. Box containment can then be used to derive both the posterior\nprobability of a mention exhibiting a given type and the conditional\nprobability relations between types themselves. We compare our approach with a\nvector-based typing model and observe state-of-the-art performance on several\nentity typing benchmarks. In addition to competitive typing performance, our\nbox-based model shows better performance in prediction consistency (predicting\na supertype and a subtype together) and confidence (i.e., calibration),\ndemonstrating that the box-based model captures the latent type hierarchies\nbetter than the vector-based model does.\n"}
{"id":"2101.00371","authors":"Yue Dong, Chandra Bhagavatula, Ximing Lu, Jena D. Hwang, Antoine\n  Bosselut, Jackie Chi Kit Cheung, Yejin Choi","title":"On-the-Fly Attention Modulation for Neural Generation","abstract":"  Despite considerable advancements with deep neural language models (LMs),\nneural text generation still suffers from degeneration: the generated text is\nrepetitive, generic, self-contradictory, and often lacks commonsense. Our\nanalyses on sentence-level attention patterns in LMs reveal that neural\ndegeneration may be associated with insufficient learning of task-specific\ncharacteristics by the attention mechanism. This finding motivates on-the-fly\nattention modulation -- a simple but effective method that enables the\ninjection of priors into attention computation during inference. Automatic and\nhuman evaluation results on three text generation benchmarks demonstrate that\nattention modulation helps LMs generate text with enhanced fluency, creativity,\nand commonsense reasoning, in addition to significantly reduce sentence-level\nrepetition.\n"}
{"id":"2101.00376","authors":"Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, Xiang Ren","title":"RiddleSense: Reasoning about Riddle Questions Featuring Linguistic\n  Creativity and Commonsense Knowledge","abstract":"  Question: I have five fingers but I am not alive. What am I? Answer: a glove.\nAnswering such a riddle-style question is a challenging cognitive process, in\nthat it requires complex commonsense reasoning abilities, an understanding of\nfigurative language, and counterfactual reasoning skills, which are all\nimportant abilities for advanced natural language understanding (NLU). However,\nthere are currently no dedicated datasets aiming to test these abilities.\nHerein, we present RiddleSense, a new multiple-choice question answering task,\nwhich comes with the first large dataset (5.7k examples) for answering\nriddle-style commonsense questions. We systematically evaluate a wide range of\nmodels over the challenge, and point out that there is a large gap between the\nbest-supervised model and human performance -- suggesting intriguing future\nresearch in the direction of higher-order commonsense reasoning and linguistic\ncreativity towards building advanced NLU systems.\n"}
{"id":"2101.00379","authors":"Sharon Levy, Michael Saxon, William Yang Wang","title":"Investigating Memorization of Conspiracy Theories in Text Generation","abstract":"  The adoption of natural language generation (NLG) models can leave\nindividuals vulnerable to the generation of harmful information memorized by\nthe models, such as conspiracy theories. While previous studies examine\nconspiracy theories in the context of social media, they have not evaluated\ntheir presence in the new space of generative language models. In this work, we\ninvestigate the capability of language models to generate conspiracy theory\ntext. Specifically, we aim to answer: can we test pretrained generative\nlanguage models for the memorization and elicitation of conspiracy theories\nwithout access to the model's training data? We highlight the difficulties of\nthis task and discuss it in the context of memorization, generalization, and\nhallucination. Utilizing a new dataset consisting of conspiracy theory topics\nand machine-generated conspiracy theories helps us discover that many\nconspiracy theories are deeply rooted in the pretrained language models. Our\nexperiments demonstrate a relationship between model parameters such as size\nand temperature and their propensity to generate conspiracy theory text. These\nresults indicate the need for a more thorough review of NLG applications before\nrelease and an in-depth discussion of the drawbacks of memorization in\ngenerative language models.\n"}
{"id":"2101.00387","authors":"Jui Shah, Yaman Kumar Singla, Changyou Chen, Rajiv Ratn Shah","title":"What all do audio transformer models hear? Probing Acoustic\n  Representations for Language Delivery and its Structure","abstract":"  In recent times, BERT based transformer models have become an inseparable\npart of the 'tech stack' of text processing models. Similar progress is being\nobserved in the speech domain with a multitude of models observing\nstate-of-the-art results by using audio transformer models to encode speech.\nThis begs the question of what are these audio transformer models learning.\nMoreover, although the standard methodology is to choose the last layer\nembedding for any downstream task, but is it the optimal choice? We try to\nanswer these questions for the two recent audio transformer models, Mockingjay\nand wave2vec2.0. We compare them on a comprehensive set of language delivery\nand structure features including audio, fluency and pronunciation features.\nAdditionally, we probe the audio models' understanding of textual surface,\nsyntax, and semantic features and compare them to BERT. We do this over\nexhaustive settings for native, non-native, synthetic, read and spontaneous\nspeech datasets\n"}
{"id":"2101.00388","authors":"Houjin Yu, Xian-Ling Mao, Zewen Chi, Wei Wei and Heyan Huang","title":"A Robust and Domain-Adaptive Approach for Low-Resource Named Entity\n  Recognition","abstract":"  Recently, it has attracted much attention to build reliable named entity\nrecognition (NER) systems using limited annotated data. Nearly all existing\nworks heavily rely on domain-specific resources, such as external lexicons and\nknowledge bases. However, such domain-specific resources are often not\navailable, meanwhile it's difficult and expensive to construct the resources,\nwhich has become a key obstacle to wider adoption. To tackle the problem, in\nthis work, we propose a novel robust and domain-adaptive approach RDANER for\nlow-resource NER, which only uses cheap and easily obtainable resources.\nExtensive experiments on three benchmark datasets demonstrate that our approach\nachieves the best performance when only using cheap and easily obtainable\nresources, and delivers competitive results against state-of-the-art methods\nwhich use difficultly obtainable domainspecific resources. All our code and\ncorpora can be found on https:\/\/github.com\/houking-can\/RDANER.\n"}
{"id":"2101.00389","authors":"Alexander Spangher, Jonathan May, Sz-rung Shiang and Lingjia Deng","title":"Multitask Learning for Class-Imbalanced Discourse Classification","abstract":"  Small class-imbalanced datasets, common in many high-level semantic tasks\nlike discourse analysis, present a particular challenge to current\ndeep-learning architectures. In this work, we perform an extensive analysis on\nsentence-level classification approaches for the News Discourse dataset, one of\nthe largest high-level semantic discourse datasets recently published. We show\nthat a multitask approach can improve 7% Micro F1-score upon current\nstate-of-the-art benchmarks, due in part to label corrections across tasks,\nwhich improve performance for underrepresented classes. We also offer a\ncomparative review of additional techniques proposed to address resource-poor\nproblems in NLP, and show that none of these approaches can improve\nclassification accuracy in such a setting.\n"}
{"id":"2101.00390","authors":"Changhan Wang, Morgane Rivi\\`ere, Ann Lee, Anne Wu, Chaitanya\n  Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, Emmanuel Dupoux","title":"VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation\n  Learning, Semi-Supervised Learning and Interpretation","abstract":"  We introduce VoxPopuli, a large-scale multilingual corpus providing 100K\nhours of unlabelled speech data in 23 languages. It is the largest open data to\ndate for unsupervised representation learning as well as semi-supervised\nlearning. VoxPopuli also contains 1.8K hours of transcribed speeches in 16\nlanguages and their aligned oral interpretations into 5 other languages\ntotaling 5.1K hours. We provide speech recognition baselines and validate the\nversatility of VoxPopuli unlabelled data in semi-supervised learning under\nchallenging out-of-domain settings. We will release the corpus at\nhttps:\/\/github.com\/facebookresearch\/voxpopuli under an open license.\n"}
{"id":"2101.00391","authors":"Najoung Kim, Ellie Pavlick, Burcu Karagol Ayan, Deepak Ramachandran","title":"Which Linguist Invented the Lightbulb? Presupposition Verification for\n  Question-Answering","abstract":"  Many Question-Answering (QA) datasets contain unanswerable questions, but\ntheir treatment in QA systems remains primitive. Our analysis of the Natural\nQuestions (Kwiatkowski et al. 2019) dataset reveals that a substantial portion\nof unanswerable questions ($\\sim$21%) can be explained based on the presence of\nunverifiable presuppositions. We discuss the shortcomings of current models in\nhandling such questions, and describe how an improved system could handle them.\nThrough a user preference study, we demonstrate that the oracle behavior of our\nproposed system that provides responses based on presupposition failure is\npreferred over the oracle behavior of existing QA systems. Then we discuss how\nour proposed system could be implemented, presenting a novel framework that\nbreaks down the problem into three steps: presupposition generation,\npresupposition verification and explanation generation. We report our progress\nin tackling each subproblem, and present a preliminary approach to integrating\nthese steps into an existing QA system. We find that adding presuppositions and\ntheir verifiability to an existing model yields modest gains in downstream\nperformance and unanswerability detection. The biggest bottleneck is the\nverification component, which needs to be substantially improved for the\nintegrated system to approach ideal behavior -- even transfer from the best\nentailment models currently falls short.\n"}
{"id":"2101.00394","authors":"Hao Fei, Meishan Zhang, Bobo Li, Donghong Ji","title":"End-to-end Semantic Role Labeling with Neural Transition-based Model","abstract":"  End-to-end semantic role labeling (SRL) has been received increasing\ninterest. It performs the two subtasks of SRL: predicate identification and\nargument role labeling, jointly. Recent work is mostly focused on graph-based\nneural models, while the transition-based framework with neural networks which\nhas been widely used in a number of closely-related tasks, has not been studied\nfor the joint task yet. In this paper, we present the first work of\ntransition-based neural models for end-to-end SRL. Our transition model\nincrementally discovers all sentential predicates as well as their arguments by\na set of transition actions. The actions of the two subtasks are executed\nmutually for full interactions. Besides, we suggest high-order compositions to\nextract non-local features, which can enhance the proposed transition model\nfurther. Experimental results on CoNLL09 and Universal Proposition Bank show\nthat our final model can produce state-of-the-art performance, and meanwhile\nkeeps highly efficient in decoding. We also conduct detailed experimental\nanalysis for a deep understanding of our proposed model.\n"}
{"id":"2101.00396","authors":"Wei Zhu, Daniel Cheung","title":"Lex-BERT: Enhancing BERT based NER with lexicons","abstract":"  In this work, we represent Lex-BERT, which incorporates the lexicon\ninformation into Chinese BERT for named entity recognition (NER) tasks in a\nnatural manner. Instead of using word embeddings and a newly designed\ntransformer layer as in FLAT, we identify the boundary of words in the\nsentences using special tokens, and the modified sentence will be encoded\ndirectly by BERT. Our model does not introduce any new parameters and are more\nefficient than FLAT. In addition, we do not require any word embeddings\naccompanying the lexicon collection. Experiments on Ontonotes and ZhCrossNER\nshow that our model outperforms FLAT and other baselines.\n"}
{"id":"2101.00403","authors":"Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Sch\\\"utze","title":"Superbizarre Is Not Superb: Derivational Morphology Improves BERT's\n  Interpretation of Complex Words","abstract":"  How does the input segmentation of pretrained language models (PLMs) affect\ntheir interpretations of complex words? We present the first study\ninvestigating this question, taking BERT as the example PLM and focusing on its\nsemantic representations of English derivatives. We show that PLMs can be\ninterpreted as serial dual-route models, i.e., the meanings of complex words\nare either stored or else need to be computed from the subwords, which implies\nthat maximally meaningful input tokens should allow for the best generalization\non new words. This hypothesis is confirmed by a series of semantic probing\ntasks on which DelBERT (Derivation leveraging BERT), a model with derivational\ninput segmentation, substantially outperforms BERT with WordPiece segmentation.\nOur results suggest that the generalization capabilities of PLMs could be\nfurther improved if a morphologically-informed vocabulary of input tokens were\nused.\n"}
{"id":"2101.00406","authors":"Avi Caciularu, Arman Cohan, Iz Beltagy, Matthew E. Peters, Arie\n  Cattan, Ido Dagan","title":"CDLM: Cross-Document Language Modeling","abstract":"  We introduce a new pretraining approach geared for multi-document language\nmodeling, incorporating two key ideas into the masked language modeling\nself-supervised objective. First, instead of considering documents in\nisolation, we pretrain over sets of multiple related documents, encouraging the\nmodel to learn cross-document relationships. Second, we improve over recent\nlong-range transformers by introducing dynamic global attention that has access\nto the entire input to predict masked tokens. We release CDLM (Cross-Document\nLanguage Model), a new general language model for multi-document setting that\ncan be easily applied to downstream tasks. Our extensive analysis shows that\nboth ideas are essential for the success of CDLM, and work in synergy to set\nnew state-of-the-art results for several multi-text tasks. Code and models are\navailable at https:\/\/github.com\/aviclu\/CDLM.\n"}
{"id":"2101.00408","authors":"Devendra Singh Sachan and Mostofa Patwary and Mohammad Shoeybi and\n  Neel Kant and Wei Ping and William L Hamilton and Bryan Catanzaro","title":"End-to-End Training of Neural Retrievers for Open-Domain Question\n  Answering","abstract":"  Recent work on training neural retrievers for open-domain question answering\n(OpenQA) has employed both supervised and unsupervised approaches. However, it\nremains unclear how unsupervised and supervised methods can be used most\neffectively for neural retrievers. In this work, we systematically study\nretriever pre-training. We first propose an approach of unsupervised\npre-training with the Inverse Cloze Task and masked salient spans, followed by\nsupervised finetuning using question-context pairs. This approach leads to\nabsolute gains of 2+ points over the previous best result in the top-20\nretrieval accuracy on Natural Questions and TriviaQA datasets.\n  We also explore two approaches for end-to-end supervised training of the\nreader and retriever components in OpenQA models. In the first approach, the\nreader considers each retrieved document separately while in the second\napproach, the reader considers all the retrieved documents together. Our\nexperiments demonstrate the effectiveness of these approaches as we obtain new\nstate-of-the-art results. On the Natural Questions dataset, we obtain a top-20\nretrieval accuracy of 84, an improvement of 5 points over the recent DPR model.\nIn addition, we achieve good results on answer extraction, outperforming recent\nmodels like REALM and RAG by 3+ points. We further scale up end-to-end training\nto large models and show consistent gains in performance over smaller models.\n"}
{"id":"2101.00411","authors":"Haoyue Shi, Karen Livescu, Kevin Gimpel","title":"Substructure Substitution: Structured Data Augmentation for NLP","abstract":"  We study a family of data augmentation methods, substructure substitution\n(SUB2), for natural language processing (NLP) tasks. SUB2 generates new\nexamples by substituting substructures (e.g., subtrees or subsequences) with\nones with the same label, which can be applied to many structured NLP tasks\nsuch as part-of-speech tagging and parsing. For more general tasks (e.g., text\nclassification) which do not have explicitly annotated substructures, we\npresent variations of SUB2 based on constituency parse trees, introducing\nstructure-aware data augmentation methods to general NLP tasks. For most cases,\ntraining with the augmented dataset by SUB2 achieves better performance than\ntraining with the original training set. Further experiments show that SUB2 has\nmore consistent performance than other investigated augmentation methods,\nacross different tasks and sizes of the seed dataset.\n"}
{"id":"2101.00416","authors":"Wangchunshu Zhou, Tao Ge, Canwen Xu, Ke Xu, Furu Wei","title":"Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting","abstract":"  In this paper, we generalize text infilling (e.g., masked language models) by\nproposing Sequence Span Rewriting (SSR) as a self-supervised\nsequence-to-sequence (seq2seq) pre-training objective. SSR provides more\nfine-grained learning signals for text representations by supervising the model\nto rewrite imperfect spans to ground truth, and it is more consistent than text\ninfilling with many downstream seq2seq tasks that rewrite a source sentences\ninto a target sentence. Our experiments with T5 models on various seq2seq tasks\nshow that SSR can substantially improve seq2seq pre-training. Moreover, we\nobserve SSR is especially helpful to improve pre-training a small-size seq2seq\nmodel with a powerful imperfect span generator, which indicates a new\nperspective of transferring knowledge from a large model to a smaller model for\nseq2seq pre-training.\n"}
{"id":"2101.00419","authors":"Yiran Xing, Zai Shi, Zhao Meng, Gerhard Lakemeyer, Yunpu Ma, Roger\n  Wattenhofer","title":"KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense\n  Generation","abstract":"  We present Knowledge Enhanced Multimodal BART (KM-BART), which is a\nTransformer-based sequence-to-sequence model capable of reasoning about\ncommonsense knowledge from multimodal inputs of images and texts. We adapt the\ngenerative BART architecture to a multimodal model with visual and textual\ninputs. We further develop novel pretraining tasks to improve the model\nperformance on the Visual Commonsense Generation (VCG) task. In particular, our\npretraining task of Knowledge-based Commonsense Generation (KCG) boosts model\nperformance on the VCG task by leveraging commonsense knowledge from a large\nlanguage model pretrained on external commonsense knowledge graphs. To the best\nof our knowledge, we are the first to propose a dedicated task for improving\nmodel performance on the VCG task. Experimental results show that our model\nreaches state-of-the-art performance on the VCG task by applying these novel\npretraining tasks.\n"}
{"id":"2101.00420","authors":"Qinyuan Ye, Xiang Ren","title":"Learning to Generate Task-Specific Adapters from Task Description","abstract":"  Pre-trained text-to-text transformers such as BART have achieved impressive\nperformance across a range of NLP tasks. Recent study further shows that they\ncan learn to generalize to novel tasks, by including task descriptions as part\nof the source sequence and training the model with (source, target) examples.\nAt test time, these fine-tuned models can make inferences on new tasks using\nthe new task descriptions as part of the input. However, this approach has\npotential limitations, as the model learns to solve individual (source, target)\nexamples (i.e., at the instance level), instead of learning to solve tasks by\ntaking all examples within a task as a whole (i.e., at the task level). To this\nend, we introduce Hypter, a framework that improves text-to-text transformer's\ngeneralization ability to unseen tasks by training a hypernetwork to generate\ntask-specific, light-weight adapters from task descriptions. Experiments on\nZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves\nupon fine-tuning baselines. Notably, when using BART-Large as the main network,\nHypter brings 11.3% comparative improvement on ZEST dataset.\n"}
{"id":"2101.00421","authors":"Nikolay Bogoychev and Pinzhen Chen","title":"The Highs and Lows of Simple Lexical Domain Adaptation Approaches for\n  Neural Machine Translation","abstract":"  Machine translation systems are vulnerable to domain mismatch, especially in\na low-resource scenario. Out-of-domain translations are often of poor quality\nand prone to hallucinations, due to exposure bias and the decoder acting as a\nlanguage model. We adopt two approaches to alleviate this problem: lexical\nshortlisting restricted by IBM statistical alignments, and hypothesis\nre-ranking based on similarity. The methods are computationally cheap, widely\nknown, but not extensively experimented on domain adaptation. We demonstrate\nsuccess on low-resource out-of-domain test sets, however, the methods are\nineffective when there is sufficient data or too great domain mismatch. This is\ndue to both the IBM model losing its advantage over the implicitly learned\nneural alignment, and issues with subword segmentation of out-of-domain words.\n"}
{"id":"2101.00430","authors":"Abu Awal Md Shoeb and Gerard de Melo","title":"Assessing Emoji Use in Modern Text Processing Tools","abstract":"  Emojis have become ubiquitous in digital communication, due to their visual\nappeal as well as their ability to vividly convey human emotion, among other\nfactors. The growing prominence of emojis in social media and other instant\nmessaging also leads to an increased need for systems and tools to operate on\ntext containing emojis. In this study, we assess this support by considering\ntest sets of tweets with emojis, based on which we perform a series of\nexperiments investigating the ability of prominent NLP and text processing\ntools to adequately process them. In particular, we consider tokenization,\npart-of-speech tagging, as well as sentiment analysis. Our findings show that\nmany tools still have notable shortcomings when operating on text containing\nemojis.\n"}
{"id":"2101.00433","authors":"Michael Saxon, Sharon Levy, Xinyi Wang, Alon Albalak, William Yang\n  Wang","title":"Modeling Disclosive Transparency in NLP Application Descriptions","abstract":"  Broader disclosive transparency$-$truth and clarity in communication\nregarding the function of AI systems$-$is widely considered desirable.\nUnfortunately, it is a nebulous concept, difficult to both define and quantify.\nThis is problematic, as previous work has demonstrated possible trade-offs and\nnegative consequences to disclosive transparency, such as a confusion effect,\nwhere \"too much information\" clouds a reader's understanding of what a system\ndescription means. Disclosive transparency's subjective nature has rendered\ndeep study into these problems and their remedies difficult. To improve this\nstate of affairs, We introduce neural language model-based probabilistic\nmetrics to directly model disclosive transparency, and demonstrate that they\ncorrelate with user and expert opinions of system transparency, making them a\nvalid objective proxy. Finally, we demonstrate the use of these metrics in a\npilot study quantifying the relationships between transparency, confusion, and\nuser perceptions in a corpus of real NLP system descriptions.\n"}
{"id":"2101.00434","authors":"Yuval Kirstain, Ori Ram, Omer Levy","title":"Coreference Resolution without Span Representations","abstract":"  The introduction of pretrained language models has reduced many complex\ntask-specific NLP models to simple lightweight layers. An exception to this\ntrend is coreference resolution, where a sophisticated task-specific model is\nappended to a pretrained transformer encoder. While highly effective, the model\nhas a very large memory footprint -- primarily due to dynamically-constructed\nspan and span-pair representations -- which hinders the processing of complete\ndocuments and the ability to train on multiple instances in a single batch. We\nintroduce a lightweight end-to-end coreference model that removes the\ndependency on span representations, handcrafted features, and heuristics. Our\nmodel performs competitively with the current standard model, while being\nsimpler and more efficient.\n"}
{"id":"2101.00436","authors":"Omar Khattab, Christopher Potts, Matei Zaharia","title":"Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval","abstract":"  Multi-hop reasoning (i.e., reasoning across two or more documents) is a key\ningredient for NLP models that leverage large corpora to exhibit broad\nknowledge. To retrieve evidence passages, multi-hop models must contend with a\nfast-growing search space across the hops, represent complex queries that\ncombine multiple information needs, and resolve ambiguity about the best order\nin which to hop between training passages. We tackle these problems via Baleen,\na system that improves the accuracy of multi-hop retrieval while learning\nrobustly from weak training signals in the many-hop setting. To tame the search\nspace, we propose condensed retrieval, a pipeline that summarizes the retrieved\npassages after each hop into a single compact context. To model complex\nqueries, we introduce a focused late interaction retriever that allows\ndifferent parts of the same query representation to match disparate relevant\npassages. Lastly, to infer the hopping dependencies among unordered training\npassages, we devise latent hop ordering, a weak-supervision strategy in which\nthe trained retriever itself selects the sequence of hops. We evaluate Baleen\non retrieval for two-hop question answering and many-hop claim verification,\nestablishing state-of-the-art performance.\n"}
{"id":"2101.00438","authors":"Ori Ram and Yuval Kirstain and Jonathan Berant and Amir Globerson and\n  Omer Levy","title":"Few-Shot Question Answering by Pretraining Span Selection","abstract":"  In several question answering benchmarks, pretrained models have reached\nhuman parity through fine-tuning on an order of 100,000 annotated questions and\nanswers. We explore the more realistic few-shot setting, where only a few\nhundred training examples are available, and observe that standard models\nperform poorly, highlighting the discrepancy between current pretraining\nobjectives and question answering. We propose a new pretraining scheme tailored\nfor question answering: recurring span selection. Given a passage with multiple\nsets of recurring spans, we mask in each set all recurring spans but one, and\nask the model to select the correct span in the passage for each masked span.\nMasked spans are replaced with a special token, viewed as a question\nrepresentation, that is later used during fine-tuning to select the answer\nspan. The resulting model obtains surprisingly good results on multiple\nbenchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while\nmaintaining competitive performance in the high-resource setting.\n"}
{"id":"2101.00529","authors":"Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang,\n  Lijuan Wang, Yejin Choi, Jianfeng Gao","title":"VinVL: Revisiting Visual Representations in Vision-Language Models","abstract":"  This paper presents a detailed study of improving visual representations for\nvision language (VL) tasks and develops an improved object detection model to\nprovide object-centric representations of images. Compared to the most widely\nused \\emph{bottom-up and top-down} model \\cite{anderson2018bottom}, the new\nmodel is bigger, better-designed for VL tasks, and pre-trained on much larger\ntraining corpora that combine multiple public annotated object detection\ndatasets. Therefore, it can generate representations of a richer collection of\nvisual objects and concepts. While previous VL research focuses mainly on\nimproving the vision-language fusion model and leaves the object detection\nmodel improvement untouched, we show that visual features matter significantly\nin VL models. In our experiments we feed the visual features generated by the\nnew object detection model into a Transformer-based VL fusion model \\oscar\n\\cite{li2020oscar}, and utilize an improved approach \\short\\ to pre-train the\nVL model and fine-tune it on a wide range of downstream VL tasks. Our results\nshow that the new visual features significantly improve the performance across\nall VL tasks, creating new state-of-the-art results on seven public benchmarks.\nWe will release the new object detection model to public.\n"}
{"id":"2101.00540","authors":"Zeming Chen","title":"Attentive Tree-structured Network for Monotonicity Reasoning","abstract":"  Many state-of-art neural models designed for monotonicity reasoning perform\npoorly on downward inference. To address this shortcoming, we developed an\nattentive tree-structured neural network. It consists of a tree-based\nlong-short-term-memory network (Tree-LSTM) with soft attention. It is designed\nto model the syntactic parse tree information from the sentence pair of a\nreasoning task. A self-attentive aggregator is used for aligning the\nrepresentations of the premise and the hypothesis. We present our model and\nevaluate it using the Monotonicity Entailment Dataset (MED). We show and\nattempt to explain that our model outperforms existing models on MED.\n"}
{"id":"2101.00542","authors":"Yanyang Li, Ye Lin, Tong Xiao, Jingbo Zhu","title":"An Efficient Transformer Decoder with Compressed Sub-layers","abstract":"  The large attention-based encoder-decoder network (Transformer) has become\nprevailing recently due to its effectiveness. But the high computation\ncomplexity of its decoder raises the inefficiency issue. By examining the\nmathematic formulation of the decoder, we show that under some mild conditions,\nthe architecture could be simplified by compressing its sub-layers, the basic\nbuilding block of Transformer, and achieves a higher parallelism. We thereby\npropose Compressed Attention Network, whose decoder layer consists of only one\nsub-layer instead of three. Extensive experiments on 14 WMT machine translation\ntasks show that our model is 1.42x faster with performance on par with a strong\nbaseline. This strong baseline is already 2x faster than the widely used\nstandard baseline without loss in performance.\n"}
{"id":"2101.00563","authors":"Sahil Sidheekh","title":"Learning Neural Networks on SVD Boosted Latent Spaces for Semantic\n  Classification","abstract":"  The availability of large amounts of data and compelling computation power\nhave made deep learning models much popular for text classification and\nsentiment analysis. Deep neural networks have achieved competitive performance\non the above tasks when trained on naive text representations such as word\ncount, term frequency, and binary matrix embeddings. However, many of the above\nrepresentations result in the input space having a dimension of the order of\nthe vocabulary size, which is enormous. This leads to a blow-up in the number\nof parameters to be learned, and the computational cost becomes infeasible when\nscaling to domains that require retaining a colossal vocabulary. This work\nproposes using singular value decomposition to transform the high dimensional\ninput space to a lower-dimensional latent space. We show that neural networks\ntrained on this lower-dimensional space are not only able to retain performance\nwhile savoring significant reduction in the computational complexity but, in\nmany situations, also outperforms the classical neural networks trained on the\nnative input space.\n"}
{"id":"2101.00674","authors":"Dennis Ulmer","title":"Recoding latent sentence representations -- Dynamic gradient-based\n  activation modification in RNNs","abstract":"  In Recurrent Neural Networks (RNNs), encoding information in a suboptimal or\nerroneous way can impact the quality of representations based on later elements\nin the sequence and subsequently lead to wrong predictions and a worse model\nperformance. In humans, challenging cases like garden path sentences (an\ninstance of this being the infamous \"The horse raced past the barn fell\") can\nlead their language understanding astray. However, they are still able to\ncorrect their representation accordingly and recover when new information is\nencountered. Inspired by this, I propose an augmentation to standard RNNs in\nform of a gradient-based correction mechanism: This way I hope to enable such\nmodels to dynamically adapt their inner representation of a sentence, adding a\nway to correct deviations as soon as they occur. This could therefore lead to\nmore robust models using more flexible representations, even during inference\ntime.\n  I conduct different experiments in the context of language modeling, where\nthe impact of using such a mechanism is examined in detail. To this end, I look\nat modifications based on different kinds of time-dependent error signals and\nhow they influence the model performance. Furthermore, this work contains a\nstudy of the model's confidence in its predictions during training and for\nchallenging test samples and the effect of the manipulation thereof. Lastly, I\nalso study the difference in behavior of these novel models compared to a\nstandard LSTM baseline and investigate error cases in detail to identify points\nof future research. I show that while the proposed approach comes with\npromising theoretical guarantees and an appealing intuition, it is only able to\nproduce minor improvements over the baseline due to challenges in its practical\napplication and the efficacy of the tested model variants.\n"}
{"id":"2101.00737","authors":"Xin Tan, Longyin Zhang and Guodong Zhou","title":"Coreference Resolution: Are the eliminated spans totally worthless?","abstract":"  Various neural-based methods have been proposed so far for joint mention\ndetection and coreference resolution. However, existing works on coreference\nresolution are mainly dependent on filtered mention representation, while other\nspans are largely neglected. In this paper, we aim at increasing the\nutilization rate of data and investigating whether those eliminated spans are\ntotally useless, or to what extent they can improve the performance of\ncoreference resolution. To achieve this, we propose a mention representation\nrefining strategy where spans highly related to mentions are well leveraged\nusing a pointer network for representation enhancing. Notably, we utilize an\nadditional loss term in this work to encourage the diversity between entity\nclusters. Experimental results on the document-level CoNLL-2012 Shared Task\nEnglish dataset show that eliminated spans are indeed much effective and our\napproach can achieve competitive results when compared with previous\nstate-of-the-art in coreference resolution.\n"}
{"id":"2101.00760","authors":"Ning Bian, Xianpei Han, Bo Chen, Le Sun","title":"Benchmarking Knowledge-Enhanced Commonsense Question Answering via\n  Knowledge-to-Text Transformation","abstract":"  A fundamental ability of humans is to utilize commonsense knowledge in\nlanguage understanding and question answering. In recent years, many\nknowledge-enhanced Commonsense Question Answering (CQA) approaches have been\nproposed. However, it remains unclear: (1) How far can we get by exploiting\nexternal knowledge for CQA? (2) How much potential of knowledge has been\nexploited in current CQA models? (3) Which are the most promising directions\nfor future CQA? To answer these questions, we benchmark knowledge-enhanced CQA\nby conducting extensive experiments on multiple standard CQA datasets using a\nsimple and effective knowledge-to-text transformation framework. Experiments\nshow that: (1) Our knowledge-to-text framework is effective and achieves\nstate-of-the-art performance on CommonsenseQA dataset, providing a simple and\nstrong knowledge-enhanced baseline for CQA; (2) The potential of knowledge is\nstill far from being fully exploited in CQA -- there is a significant\nperformance gap from current models to our models with golden knowledge; and\n(3) Context-sensitive knowledge selection, heterogeneous knowledge\nexploitation, and commonsense-rich language models are promising CQA\ndirections.\n"}
{"id":"2101.00816","authors":"Yue Mao, Yi Shen, Chao Yu, Longjun Cai","title":"A Joint Training Dual-MRC Framework for Aspect Based Sentiment Analysis","abstract":"  Aspect based sentiment analysis (ABSA) involves three fundamental subtasks:\naspect term extraction, opinion term extraction, and aspect-level sentiment\nclassification. Early works only focused on solving one of these subtasks\nindividually. Some recent work focused on solving a combination of two\nsubtasks, e.g., extracting aspect terms along with sentiment polarities or\nextracting the aspect and opinion terms pair-wisely. More recently, the triple\nextraction task has been proposed, i.e., extracting the (aspect term, opinion\nterm, sentiment polarity) triples from a sentence. However, previous approaches\nfail to solve all subtasks in a unified end-to-end framework. In this paper, we\npropose a complete solution for ABSA. We construct two machine reading\ncomprehension (MRC) problems and solve all subtasks by joint training two\nBERT-MRC models with parameters sharing. We conduct experiments on these\nsubtasks, and results on several benchmark datasets demonstrate the\neffectiveness of our proposed framework, which significantly outperforms\nexisting state-of-the-art methods.\n"}
{"id":"2101.00822","authors":"Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong, Changyou Chen","title":"Outline to Story: Fine-grained Controllable Story Generation from\n  Cascaded Events","abstract":"  Large-scale pretrained language models have shown thrilling generation\ncapabilities, especially when they generate consistent long text in thousands\nof words with ease. However, users of these models can only control the prefix\nof sentences or certain global aspects of generated text. It is challenging to\nsimultaneously achieve fine-grained controllability and preserve the\nstate-of-the-art unconditional text generation capability. In this paper, we\nfirst propose a new task named \"Outline to Story\" (O2S) as a test bed for\nfine-grained controllable generation of long text, which generates a\nmulti-paragraph story from cascaded events, i.e. a sequence of outline events\nthat guide subsequent paragraph generation. We then create dedicate datasets\nfor future benchmarks, built by state-of-the-art keyword extraction techniques.\nFinally, we propose an extremely simple yet strong baseline method for the O2S\ntask, which fine tunes pre-trained language models on augmented sequences of\noutline-story pairs with simple language modeling objective. Our method does\nnot introduce any new parameters or perform any architecture modification,\nexcept several special tokens as delimiters to build augmented sequences.\nExtensive experiments on various datasets demonstrate state-of-the-art\nconditional story generation performance with our model, achieving better\nfine-grained controllability and user flexibility. Our paper is among the first\nones by our knowledge to propose a model and to create datasets for the task of\n\"outline to story\". Our work also instantiates research interest of\nfine-grained controllable generation of open-domain long text, where\ncontrolling inputs are represented by short text.\n"}
{"id":"2101.00828","authors":"Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong, Changyou Chen","title":"Transformer-based Conditional Variational Autoencoder for Controllable\n  Story Generation","abstract":"  We investigate large-scale latent variable models (LVMs) for neural story\ngeneration -- an under-explored application for open-domain long text -- with\nobjectives in two threads: generation effectiveness and controllability. LVMs,\nespecially the variational autoencoder (VAE), have achieved both effective and\ncontrollable generation through exploiting flexible distributional latent\nrepresentations. Recently, Transformers and its variants have achieved\nremarkable effectiveness without explicit latent representation learning, thus\nlack satisfying controllability in generation. In this paper, we advocate to\nrevive latent variable modeling, essentially the power of representation\nlearning, in the era of Transformers to enhance controllability without hurting\nstate-of-the-art generation effectiveness. Specifically, we integrate latent\nrepresentation vectors with a Transformer-based pre-trained architecture to\nbuild conditional variational autoencoder (CVAE). Model components such as\nencoder, decoder and the variational posterior are all built on top of\npre-trained language models -- GPT2 specifically in this paper. Experiments\ndemonstrate state-of-the-art conditional generation ability of our model, as\nwell as its excellent representation learning capability and controllability.\n"}
{"id":"2101.00916","authors":"Li Liu, Mengge He, Guanghui Xu, Mingkui Tan, Qi Wu","title":"How to Train Your Agent to Read and Write","abstract":"  Reading and writing research papers is one of the most privileged abilities\nthat a qualified researcher should master. However, it is difficult for new\nresearchers (\\eg{students}) to fully {grasp} this ability. It would be\nfascinating if we could train an intelligent agent to help people read and\nsummarize papers, and perhaps even discover and exploit the potential knowledge\nclues to write novel papers. Although there have been existing works focusing\non summarizing (\\emph{i.e.}, reading) the knowledge in a given text or\ngenerating (\\emph{i.e.}, writing) a text based on the given knowledge, the\nability of simultaneously reading and writing is still under development.\nTypically, this requires an agent to fully understand the knowledge from the\ngiven text materials and generate correct and fluent novel paragraphs, which is\nvery challenging in practice. In this paper, we propose a Deep ReAder-Writer\n(DRAW) network, which consists of a \\textit{Reader} that can extract knowledge\ngraphs (KGs) from input paragraphs and discover potential knowledge, a\ngraph-to-text \\textit{Writer} that generates a novel paragraph, and a\n\\textit{Reviewer} that reviews the generated paragraph from three different\naspects. Extensive experiments show that our DRAW network outperforms\nconsidered baselines and several state-of-the-art methods on AGENDA and\nM-AGENDA datasets. Our code and supplementary are released at\nhttps:\/\/github.com\/menggehe\/DRAW.\n"}
{"id":"2101.00939","authors":"Kun Zhou, Xiaolei Wang, Yuanhang Zhou, Chenzhan Shang, Yuan Cheng,\n  Wayne Xin Zhao, Yaliang Li, Ji-Rong Wen","title":"CRSLab: An Open-Source Toolkit for Building Conversational Recommender\n  System","abstract":"  In recent years, conversational recommender system (CRS) has received much\nattention in the research community. However, existing studies on CRS vary in\nscenarios, goals and techniques, lacking unified, standardized implementation\nor comparison. To tackle this challenge, we propose an open-source CRS toolkit\nCRSLab, which provides a unified and extensible framework with highly-decoupled\nmodules to develop CRSs. Based on this framework, we collect 6 commonly-used\nhuman-annotated CRS datasets and implement 18 models that include recent\ntechniques such as graph neural network and pre-training models. Besides, our\ntoolkit provides a series of automatic evaluation protocols and a human-machine\ninteraction interface to test and compare different CRS methods. The project\nand documents are released at https:\/\/github.com\/RUCAIBox\/CRSLab.\n"}
{"id":"2101.01039","authors":"Ken Voskuil and Suzan Verberne","title":"Improving reference mining in patents with BERT","abstract":"  In this paper we address the challenge of extracting scientific references\nfrom patents. We approach the problem as a sequence labelling task and\ninvestigate the merits of BERT models to the extraction of these long\nsequences. References in patents to scientific literature are relevant to study\nthe connection between science and industry. Most prior work only uses the\nfront-page citations for this analysis, which are provided in the metadata of\npatent archives. In this paper we build on prior work using Conditional Random\nFields (CRF) and Flair for reference extraction. We improve the quality of the\ntraining data and train three BERT-based models on the labelled data (BERT,\nbioBERT, sciBERT). We find that the improved training data leads to a large\nimprovement in the quality of the trained models. In addition, the BERT models\nbeat CRF and Flair, with recall scores around 97% obtained with cross\nvalidation. With the best model we label a large collection of 33 thousand\npatents, extract the citations, and match them to publications in the Web of\nScience database. We extract 50% more references than with the old training\ndata and methods: 735 thousand references in total. With these\npatent-publication links, follow-up research will further analyze which types\nof scientific work lead to inventions.\n"}
{"id":"2101.01142","authors":"Michal Choras, Konstantinos Demestichas, Agata Gielczyk, Alvaro\n  Herrero, Pawel Ksieniewicz, Konstantina Remoundou, Daniel Urda, Michal\n  Wozniak","title":"Advanced Machine Learning Techniques for Fake News (Online\n  Disinformation) Detection: A Systematic Mapping Study","abstract":"  Fake news has now grown into a big problem for societies and also a major\nchallenge for people fighting disinformation. This phenomenon plagues\ndemocratic elections, reputations of individual persons or organizations, and\nhas negatively impacted citizens, (e.g., during the COVID-19 pandemic in the US\nor Brazil). Hence, developing effective tools to fight this phenomenon by\nemploying advanced Machine Learning (ML) methods poses a significant challenge.\nThe following paper displays the present body of knowledge on the application\nof such intelligent tools in the fight against disinformation. It starts by\nshowing the historical perspective and the current role of fake news in the\ninformation war. Proposed solutions based solely on the work of experts are\nanalysed and the most important directions of the application of intelligent\nsystems in the detection of misinformation sources are pointed out.\nAdditionally, the paper presents some useful resources (mainly datasets useful\nwhen assessing ML solutions for fake news detection) and provides a short\noverview of the most important R&D projects related to this subject. The main\npurpose of this work is to analyse the current state of knowledge in detecting\nfake news; on the one hand to show possible solutions, and on the other hand to\nidentify the main challenges and methodological gaps to motivate future\nresearch.\n"}
{"id":"2101.01213","authors":"Sofia Oliveira and Daniel Loureiro and Al\\'ipio Jorge","title":"Improving Portuguese Semantic Role Labeling with Transformers and\n  Transfer Learning","abstract":"  The Natural Language Processing task of determining \"Who did what to whom\" is\ncalled Semantic Role Labeling. For English, recent methods based on Transformer\nmodels have allowed for major improvements in this task over the previous state\nof the art. However, for low resource languages, like Portuguese, currently\navailable semantic role labeling models are hindered by scarce training data.\nIn this paper, we explore a model architecture with only a pre-trained\nTransformer-based model, a linear layer, softmax and Viterbi decoding. We\nsubstantially improve the state-of-the-art performance in Portuguese by over 15\nF1. Additionally, we improve semantic role labeling results in Portuguese\ncorpora by exploiting cross-lingual transfer learning using multilingual\npre-trained models, and transfer learning from dependency parsing in\nPortuguese, evaluating the various proposed approaches empirically.\n"}
{"id":"2101.01228","authors":"Nicholas Botzer, Yifan Ding, Tim Weninger","title":"Reddit Entity Linking Dataset","abstract":"  We introduce and make publicly available an entity linking dataset from\nReddit that contains 17,316 linked entities, each annotated by three human\nannotators and then grouped into Gold, Silver, and Bronze to indicate\ninter-annotator agreement. We analyze the different errors and disagreements\nmade by annotators and suggest three types of corrections to the raw data.\nFinally, we tested existing entity linking models that are trained and tuned on\ntext from non-social media datasets. We find that, although these existing\nentity linking models perform very well on their original datasets, they\nperform poorly on this social media dataset. We also show that the majority of\nthese errors can be attributed to poor performance on the mention detection\nsubtask. These results indicate the need for better entity linking models that\ncan be applied to the enormous amount of social media text.\n"}
{"id":"2101.01321","authors":"Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer","title":"I-BERT: Integer-only BERT Quantization","abstract":"  Transformer based models, like BERT and RoBERTa, have achieved\nstate-of-the-art results in many Natural Language Processing tasks. However,\ntheir memory footprint, inference latency, and power consumption are\nprohibitive efficient inference at the edge, and even at the data center. While\nquantization can be a viable solution for this, previous work on quantizing\nTransformer based models use floating-point arithmetic during inference, which\ncannot efficiently utilize integer-only logical units such as the recent Turing\nTensor Cores, or traditional integer-only ARM processors. In this work, we\npropose I-BERT, a novel quantization scheme for Transformer based models that\nquantizes the entire inference with integer-only arithmetic. Based on\nlightweight integer-only approximation methods for nonlinear operations, e.g.,\nGELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end\ninteger-only BERT inference without any floating point calculation. We evaluate\nour approach on GLUE downstream tasks using RoBERTa-Base\/Large. We show that\nfor both cases, I-BERT achieves similar (and slightly higher) accuracy as\ncompared to the full-precision baseline. Furthermore, our preliminary\nimplementation of I-BERT shows a speedup of 2.4-4.0x for INT8 inference on a T4\nGPU system as compared to FP32 inference. The framework has been developed in\nPyTorch and has been open-sourced.\n"}
{"id":"2101.01334","authors":"Akshay Agarwal, Shashank Maiya, Sonu Aggarwal","title":"Evaluating Empathetic Chatbots in Customer Service Settings","abstract":"  Customer service is a setting that calls for empathy in live human agent\nresponses. Recent advances have demonstrated how open-domain chatbots can be\ntrained to demonstrate empathy when responding to live human utterances. We\nshow that a blended skills chatbot model that responds to customer queries is\nmore likely to resemble actual human agent response if it is trained to\nrecognize emotion and exhibit appropriate empathy, than a model without such\ntraining. For our analysis, we leverage a Twitter customer service dataset\ncontaining several million customer<->agent dialog examples in customer service\ncontexts from 20 well-known brands.\n"}
{"id":"2101.01337","authors":"Mohammed Alawad, Shang Gao, Mayanka Chandra Shekar, S.M.Shamimul\n  Hasan, J. Blair Christian, Xiao-Cheng Wu, Eric B. Durbin, Jennifer Doherty,\n  Antoinette Stroup, Linda Coyle, Lynne Penberthy, Georgia Tourassi","title":"Integration of Domain Knowledge using Medical Knowledge Graph Deep\n  Learning for Cancer Phenotyping","abstract":"  A key component of deep learning (DL) for natural language processing (NLP)\nis word embeddings. Word embeddings that effectively capture the meaning and\ncontext of the word that they represent can significantly improve the\nperformance of downstream DL models for various NLP tasks. Many existing word\nembeddings techniques capture the context of words based on word co-occurrence\nin documents and text; however, they often cannot capture broader\ndomain-specific relationships between concepts that may be crucial for the NLP\ntask at hand. In this paper, we propose a method to integrate external\nknowledge from medical terminology ontologies into the context captured by word\nembeddings. Specifically, we use a medical knowledge graph, such as the unified\nmedical language system (UMLS), to find connections between clinical terms in\ncancer pathology reports. This approach aims to minimize the distance between\nconnected clinical concepts. We evaluate the proposed approach using a\nMultitask Convolutional Neural Network (MT-CNN) to extract six cancer\ncharacteristics -- site, subsite, laterality, behavior, histology, and grade --\nfrom a dataset of ~900K cancer pathology reports. The results show that the\nMT-CNN model which uses our domain informed embeddings outperforms the same\nMT-CNN using standard word2vec embeddings across all tasks, with an improvement\nin the overall micro- and macro-F1 scores by 4.97\\%and 22.5\\%, respectively.\n"}
{"id":"2101.01353","authors":"Weixin Zeng, Xiang Zhao, Jiuyang Tang, Xuemin Lin and Paul Groth","title":"Reinforcement Learning based Collective Entity Alignment with Adaptive\n  Features","abstract":"  Entity alignment (EA) is the task of identifying the entities that refer to\nthe same real-world object but are located in different knowledge graphs (KGs).\nFor entities to be aligned, existing EA solutions treat them separately and\ngenerate alignment results as ranked lists of entities on the other side.\nNevertheless, this decision-making paradigm fails to take into account the\ninterdependence among entities. Although some recent efforts mitigate this\nissue by imposing the 1-to-1 constraint on the alignment process, they still\ncannot adequately model the underlying interdependence and the results tend to\nbe sub-optimal. To fill in this gap, in this work, we delve into the dynamics\nof the decision-making process, and offer a reinforcement learning (RL) based\nmodel to align entities collectively. Under the RL framework, we devise the\ncoherence and exclusiveness constraints to characterize the interdependence and\nrestrict collective alignment. Additionally, to generate more precise inputs to\nthe RL framework, we employ representative features to capture different\naspects of the similarity between entities in heterogeneous KGs, which are\nintegrated by an adaptive feature fusion strategy. Our proposal is evaluated on\nboth cross-lingual and mono-lingual EA benchmarks and compared against\nstate-of-the-art solutions. The empirical results verify its effectiveness and\nsuperiority.\n"}
{"id":"2101.01391","authors":"Ruibo Liu, Lili Wang, Chenyan Jia, Soroush Vosoughi","title":"Political Depolarization of News Articles Using Attribute-aware Word\n  Embeddings","abstract":"  Political polarization in the US is on the rise. This polarization negatively\naffects the public sphere by contributing to the creation of ideological echo\nchambers. In this paper, we focus on addressing one of the factors that\ncontributes to this polarity, polarized media. We introduce a framework for\ndepolarizing news articles. Given an article on a certain topic with a\nparticular ideological slant (eg., liberal or conservative), the framework\nfirst detects polar language in the article and then generates a new article\nwith the polar language replaced with neutral expressions. To detect polar\nwords, we train a multi-attribute-aware word embedding model that is aware of\nideology and topics on 360k full-length media articles. Then, for text\ngeneration, we propose a new algorithm called Text Annealing Depolarization\nAlgorithm (TADA). TADA retrieves neutral expressions from the word embedding\nmodel that not only decrease ideological polarity but also preserve the\noriginal argument of the text, while maintaining grammatical correctness. We\nevaluate our framework by comparing the depolarized output of our model in two\nmodes, fully-automatic and semi-automatic, on 99 stories spanning 11 topics.\nBased on feedback from 161 human testers, our framework successfully\ndepolarized 90.1% of paragraphs in semi-automatic mode and 78.3% of paragraphs\nin fully-automatic mode. Furthermore, 81.2% of the testers agree that the\nnon-polar content information is well-preserved and 79% agree that\ndepolarization does not harm semantic correctness when they compare the\noriginal text and the depolarized text. Our work shows that data-driven methods\ncan help to locate political polarity and aid in the depolarization of\narticles.\n"}
{"id":"2101.01447","authors":"Hung-Ting Su, Chen-Hsi Chang, Po-Wei Shen, Yu-Siang Wang, Ya-Liang\n  Chang, Yu-Cheng Chang, Pu-Jen Cheng and Winston H. Hsu","title":"End-to-End Video Question-Answer Generation with Generator-Pretester\n  Network","abstract":"  We study a novel task, Video Question-Answer Generation (VQAG), for\nchallenging Video Question Answering (Video QA) task in multimedia. Due to\nexpensive data annotation costs, many widely used, large-scale Video QA\ndatasets such as Video-QA, MSVD-QA and MSRVTT-QA are automatically annotated\nusing Caption Question Generation (CapQG) which inputs captions instead of the\nvideo itself. As captions neither fully represent a video, nor are they always\npractically available, it is crucial to generate question-answer pairs based on\na video via Video Question-Answer Generation (VQAG). Existing video-to-text\n(V2T) approaches, despite taking a video as the input, only generate a question\nalone. In this work, we propose a novel model Generator-Pretester Network that\nfocuses on two components: (1) The Joint Question-Answer Generator (JQAG) which\ngenerates a question with its corresponding answer to allow Video Question\n\"Answering\" training. (2) The Pretester (PT) verifies a generated question by\ntrying to answer it and checks the pretested answer with both the model's\nproposed answer and the ground truth answer. We evaluate our system with the\nonly two available large-scale human-annotated Video QA datasets and achieves\nstate-of-the-art question generation performances. Furthermore, using our\ngenerated QA pairs only on the Video QA task, we can surpass some supervised\nbaselines. We apply our generated questions to Video QA applications and\nsurpasses some supervised baselines using generated questions only. As a\npre-training strategy, we outperform both CapQG and transfer learning\napproaches when employing semi-supervised (20%) or fully supervised learning\nwith annotated data. These experimental results suggest the novel perspectives\nfor Video QA training.\n"}
{"id":"2101.01476","authors":"Linh The Nguyen, Dat Quoc Nguyen","title":"PhoNLP: A joint multi-task learning model for Vietnamese part-of-speech\n  tagging, named entity recognition and dependency parsing","abstract":"  We present the first multi-task learning model -- named PhoNLP -- for joint\nVietnamese part-of-speech (POS) tagging, named entity recognition (NER) and\ndependency parsing. Experiments on Vietnamese benchmark datasets show that\nPhoNLP produces state-of-the-art results, outperforming a single-task learning\napproach that fine-tunes the pre-trained Vietnamese language model PhoBERT\n(Nguyen and Nguyen, 2020) for each task independently. We publicly release\nPhoNLP as an open-source toolkit under the Apache License 2.0. Although we\nspecify PhoNLP for Vietnamese, our PhoNLP training and evaluation command\nscripts in fact can directly work for other languages that have a pre-trained\nBERT-based language model and gold annotated corpora available for the three\ntasks of POS tagging, NER and dependency parsing. We hope that PhoNLP can serve\nas a strong baseline and useful toolkit for future NLP research and\napplications to not only Vietnamese but also the other languages. Our PhoNLP is\navailable at: https:\/\/github.com\/VinAIResearch\/PhoNLP\n"}
{"id":"2101.01628","authors":"David Noever, Josh Kalin, Matt Ciolino, Dom Hambrick, and Gerry Dozier","title":"Local Translation Services for Neglected Languages","abstract":"  Taking advantage of computationally lightweight, but high-quality translators\nprompt consideration of new applications that address neglected languages.\nLocally run translators for less popular languages may assist data projects\nwith protected or personal data that may require specific compliance checks\nbefore posting to a public translation API, but which could render reasonable,\ncost-effective solutions if done with an army of local, small-scale pair\ntranslators. Like handling a specialist's dialect, this research illustrates\ntranslating two historically interesting, but obfuscated languages: 1)\nhacker-speak (\"l33t\") and 2) reverse (or \"mirror\") writing as practiced by\nLeonardo da Vinci. The work generalizes a deep learning architecture to\ntranslatable variants of hacker-speak with lite, medium, and hard vocabularies.\nThe original contribution highlights a fluent translator of hacker-speak in\nunder 50 megabytes and demonstrates a generator for augmenting future datasets\nwith greater than a million bilingual sentence pairs. The long short-term\nmemory, recurrent neural network (LSTM-RNN) extends previous work demonstrating\nan English-to-foreign translation service built from as little as 10,000\nbilingual sentence pairs. This work further solves the equivalent translation\nproblem in twenty-six additional (non-obfuscated) languages and rank orders\nthose models and their proficiency quantitatively with Italian as the most\nsuccessful and Mandarin Chinese as the most challenging. For neglected\nlanguages, the method prototypes novel services for smaller niche translations\nsuch as Kabyle (Algerian dialect) which covers between 5-7 million speakers but\none which for most enterprise translators, has not yet reached development. One\nanticipates the extension of this approach to other important dialects, such as\ntranslating technical (medical or legal) jargon and processing health records.\n"}
{"id":"2101.01634","authors":"Lorenzo De Mattei, Michele Cafagna, Huiyuan Lai, Felice Dell'Orletta,\n  Malvina Nissim, Albert Gatt","title":"On the interaction of automatic evaluation and task framing in headline\n  style transfer","abstract":"  An ongoing debate in the NLG community concerns the best way to evaluate\nsystems, with human evaluation often being considered the most reliable method,\ncompared to corpus-based metrics. However, tasks involving subtle textual\ndifferences, such as style transfer, tend to be hard for humans to perform. In\nthis paper, we propose an evaluation method for this task based on\npurposely-trained classifiers, showing that it better reflects system\ndifferences than traditional metrics such as BLEU and ROUGE.\n"}
{"id":"2101.01686","authors":"Binyuan Hui, Ruiying Geng, Qiyu Ren, Binhua Li, Yongbin Li, Jian Sun,\n  Fei Huang, Luo Si, Pengfei Zhu, Xiaodan Zhu","title":"Dynamic Hybrid Relation Network for Cross-Domain Context-Dependent\n  Semantic Parsing","abstract":"  Semantic parsing has long been a fundamental problem in natural language\nprocessing. Recently, cross-domain context-dependent semantic parsing has\nbecome a new focus of research. Central to the problem is the challenge of\nleveraging contextual information of both natural language utterance and\ndatabase schemas in the interaction history. In this paper, we present a\ndynamic graph framework that is capable of effectively modelling contextual\nutterances, tokens, database schemas, and their complicated interaction as the\nconversation proceeds. The framework employs a dynamic memory decay mechanism\nthat incorporates inductive bias to integrate enriched contextual relation\nrepresentation, which is further enhanced with a powerful reranking model. At\nthe time of writing, we demonstrate that the proposed framework outperforms all\nexisting models by large margins, achieving new state-of-the-art performance on\ntwo large-scale benchmarks, the SParC and CoSQL datasets. Specifically, the\nmodel attains a 55.8% question-match and 30.8% interaction-match accuracy on\nSParC, and a 46.8% question-match and 17.0% interaction-match accuracy on\nCoSQL.\n"}
{"id":"2101.01761","authors":"Hieu Pham, Quoc V. Le","title":"AutoDropout: Learning Dropout Patterns to Regularize Deep Networks","abstract":"  Neural networks are often over-parameterized and hence benefit from\naggressive regularization. Conventional regularization methods, such as Dropout\nor weight decay, do not leverage the structures of the network's inputs and\nhidden states. As a result, these conventional methods are less effective than\nmethods that leverage the structures, such as SpatialDropout and DropBlock,\nwhich randomly drop the values at certain contiguous areas in the hidden states\nand setting them to zero. Although the locations of dropout areas random, the\npatterns of SpatialDropout and DropBlock are manually designed and fixed. Here\nwe propose to learn the dropout patterns. In our method, a controller learns to\ngenerate a dropout pattern at every channel and layer of a target network, such\nas a ConvNet or a Transformer. The target network is then trained with the\ndropout pattern, and its resulting validation performance is used as a signal\nfor the controller to learn from. We show that this method works well for both\nimage recognition on CIFAR-10 and ImageNet, as well as language modeling on\nPenn Treebank and WikiText-2. The learned dropout patterns also transfers to\ndifferent tasks and datasets, such as from language model on Penn Treebank to\nEngligh-French translation on WMT 2014. Our code will be available.\n"}
{"id":"2101.01775","authors":"Yu Chen, Ananya Subburathinam, Ching-Hua Chen and Mohammed J. Zaki","title":"Personalized Food Recommendation as Constrained Question Answering over\n  a Large-scale Food Knowledge Graph","abstract":"  Food recommendation has become an important means to help guide users to\nadopt healthy dietary habits. Previous works on food recommendation either i)\nfail to consider users' explicit requirements, ii) ignore crucial health\nfactors (e.g., allergies and nutrition needs), or iii) do not utilize the rich\nfood knowledge for recommending healthy recipes. To address these limitations,\nwe propose a novel problem formulation for food recommendation, modeling this\ntask as constrained question answering over a large-scale food knowledge\nbase\/graph (KBQA). Besides the requirements from the user query, personalized\nrequirements from the user's dietary preferences and health guidelines are\nhandled in a unified way as additional constraints to the QA system. To\nvalidate this idea, we create a QA style dataset for personalized food\nrecommendation based on a large-scale food knowledge graph and health\nguidelines. Furthermore, we propose a KBQA-based personalized food\nrecommendation framework which is equipped with novel techniques for handling\nnegations and numerical comparisons in the queries. Experimental results on the\nbenchmark show that our approach significantly outperforms non-personalized\ncounterparts (average 59.7% absolute improvement across various evaluation\nmetrics), and is able to recommend more relevant and healthier recipes.\n"}
{"id":"2101.01785","authors":"Muhammad Abdul-Mageed, AbdelRahim Elmadany, El Moatez Billah Nagoudi","title":"ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic","abstract":"  Pre-trained language models (LMs) are currently integral to many natural\nlanguage processing systems. Although multilingual LMs were also introduced to\nserve many languages, these have limitations such as being costly at inference\ntime and the size and diversity of non-English data involved in their\npre-training. We remedy these issues for a collection of diverse Arabic\nvarieties by introducing two powerful deep bidirectional transformer-based\nmodels, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a\nnew benchmark for multi-dialectal Arabic language understanding evaluation.\nARLUE is built using 42 datasets targeting six different task clusters,\nallowing us to offer a series of standardized experiments under rich\nconditions. When fine-tuned on ARLUE, our models collectively achieve new\nstate-of-the-art results across the majority of tasks (37 out of 48\nclassification tasks, on the 42 datasets). Our best model acquires the highest\nARLUE score (77.40) across all six task clusters, outperforming all other\nmodels including XLM-R Large (~ 3.4 x larger size). Our models are publicly\navailable at https:\/\/github.com\/UBC-NLP\/marbert and ARLUE will be released\nthrough the same repository.\n"}
{"id":"2101.01853","authors":"Xuankai Chang, Naoyuki Kanda, Yashesh Gaur, Xiaofei Wang, Zhong Meng,\n  Takuya Yoshioka","title":"Hypothesis Stitcher for End-to-End Speaker-attributed ASR on Long-form\n  Multi-talker Recordings","abstract":"  An end-to-end (E2E) speaker-attributed automatic speech recognition (SA-ASR)\nmodel was proposed recently to jointly perform speaker counting, speech\nrecognition and speaker identification. The model achieved a low\nspeaker-attributed word error rate (SA-WER) for monaural overlapped speech\ncomprising an unknown number of speakers. However, the E2E modeling approach is\nsusceptible to the mismatch between the training and testing conditions. It has\nyet to be investigated whether the E2E SA-ASR model works well for recordings\nthat are much longer than samples seen during training. In this work, we first\napply a known decoding technique that was developed to perform single-speaker\nASR for long-form audio to our E2E SA-ASR task. Then, we propose a novel method\nusing a sequence-to-sequence model, called hypothesis stitcher. The model takes\nmultiple hypotheses obtained from short audio segments that are extracted from\nthe original long-form input, and it then outputs a fused single hypothesis. We\npropose several architectural variations of the hypothesis stitcher model and\ncompare them with the conventional decoding methods. Experiments using\nLibriSpeech and LibriCSS corpora show that the proposed method significantly\nimproves SA-WER especially for long-form multi-talker recordings.\n"}
{"id":"2101.01880","authors":"Sugam Garg, Harichandana and Sumit Kumar","title":"On-Device Document Classification using multimodal features","abstract":"  From small screenshots to large videos, documents take up a bulk of space in\na modern smartphone. Documents in a phone can accumulate from various sources,\nand with the high storage capacity of mobiles, hundreds of documents are\naccumulated in a short period. However, searching or managing documents remains\nan onerous task, since most search methods depend on meta-information or only\ntext in a document. In this paper, we showcase that a single modality is\ninsufficient for classification and present a novel pipeline to classify\ndocuments on-device, thus preventing any private user data transfer to server.\nFor this task, we integrate an open-source library for Optical Character\nRecognition (OCR) and our novel model architecture in the pipeline. We optimise\nthe model for size, a necessary metric for on-device inference. We benchmark\nour classification model with a standard multimodal dataset FOOD-101 and\nshowcase competitive results with the previous State of the Art with 30% model\ncompression.\n"}
{"id":"2101.01896","authors":"Jieyu Zhang, Xiangchen Song, Ying Zeng, Jiaze Chen, Jiaming Shen,\n  Yuning Mao, Lei Li","title":"Taxonomy Completion via Triplet Matching Network","abstract":"  Automatically constructing taxonomy finds many applications in e-commerce and\nweb search. One critical challenge is as data and business scope grow in real\napplications, new concepts are emerging and needed to be added to the existing\ntaxonomy. Previous approaches focus on the taxonomy expansion, i.e. finding an\nappropriate hypernym concept from the taxonomy for a new query concept. In this\npaper, we formulate a new task, \"taxonomy completion\", by discovering both the\nhypernym and hyponym concepts for a query. We propose Triplet Matching Network\n(TMN), to find the appropriate <hypernym, hyponym> pairs for a given query\nconcept. TMN consists of one primal scorer and multiple auxiliary scorers.\nThese auxiliary scorers capture various fine-grained signals (e.g., query to\nhypernym or query to hyponym semantics), and the primal scorer makes a holistic\nprediction on <query, hypernym, hyponym> triplet based on the internal feature\nrepresentations of all auxiliary scorers. Also, an innovative channel-wise\ngating mechanism that retains task-specific information in concept\nrepresentations is introduced to further boost model performance. Experiments\non four real-world large-scale datasets show that TMN achieves the best\nperformance on both taxonomy completion task and the previous taxonomy\nexpansion task, outperforming existing methods.\n"}
{"id":"2101.01907","authors":"Hailin Wang, Ke Qin, Rufai Yusuf Zakari, Guoming Lu, Jin Yin","title":"Deep Neural Network Based Relation Extraction: An Overview","abstract":"  Knowledge is a formal way of understanding the world, providing a human-level\ncognition and intelligence for the next-generation artificial intelligence\n(AI). One of the representations of knowledge is semantic relations between\nentities. An effective way to automatically acquire this important knowledge,\ncalled Relation Extraction (RE), a sub-task of information extraction, plays a\nvital role in Natural Language Processing (NLP). Its purpose is to identify\nsemantic relations between entities from natural language text. To date, there\nare several studies for RE in previous works, which have documented these\ntechniques based on Deep Neural Networks (DNNs) become a prevailing technique\nin this research. Especially, the supervised and distant supervision methods\nbased on DNNs are the most popular and reliable solutions for RE. This article\n1) introduces some general concepts, and further 2) gives a comprehensive\noverview of DNNs in RE from two points of view: supervised RE, which attempts\nto improve the standard RE systems, and distant supervision RE, which adopts\nDNNs to design sentence encoder and de-noise method. We further 3) cover some\nnovel methods and recent trends as well as discuss possible future research\ndirections for this task.\n"}
{"id":"2101.01910","authors":"Xiaopeng Lu, Kyusong Lee, Tiancheng Zhao","title":"SF-QA: Simple and Fair Evaluation Library for Open-domain Question\n  Answering","abstract":"  Although open-domain question answering (QA) draws great attention in recent\nyears, it requires large amounts of resources for building the full system and\nis often difficult to reproduce previous results due to complex configurations.\nIn this paper, we introduce SF-QA: simple and fair evaluation framework for\nopen-domain QA. SF-QA framework modularizes the pipeline open-domain QA system,\nwhich makes the task itself easily accessible and reproducible to research\ngroups without enough computing resources. The proposed evaluation framework is\npublicly available and anyone can contribute to the code and evaluations.\n"}
{"id":"2101.01926","authors":"Tongtong Wu, Xuekai Li, Yuan-Fang Li, Reza Haffari, Guilin Qi, Yujin\n  Zhu and Guoqiang Xu","title":"Curriculum-Meta Learning for Order-Robust Continual Relation Extraction","abstract":"  Continual relation extraction is an important task that focuses on extracting\nnew facts incrementally from unstructured text. Given the sequential arrival\norder of the relations, this task is prone to two serious challenges, namely\ncatastrophic forgetting and order-sensitivity. We propose a novel\ncurriculum-meta learning method to tackle the above two challenges in continual\nrelation extraction. We combine meta learning and curriculum learning to\nquickly adapt model parameters to a new task and to reduce interference of\npreviously seen tasks on the current task. We design a novel relation\nrepresentation learning method through the distribution of domain and range\ntypes of relations. Such representations are utilized to quantify the\ndifficulty of tasks for the construction of curricula. Moreover, we also\npresent novel difficulty-based metrics to quantitatively measure the extent of\norder-sensitivity of a given model, suggesting new ways to evaluate model\nrobustness. Our comprehensive experiments on three benchmark datasets show that\nour proposed method outperforms the state-of-the-art techniques. The code is\navailable at the anonymous GitHub repository:\nhttps:\/\/github.com\/wutong8023\/AAAI_CML.\n"}
{"id":"2101.02157","authors":"Sofian Chaybouti, Achraf Saghe, Aymen Shabou","title":"EfficientQA : a RoBERTa Based Phrase-Indexed Question-Answering System","abstract":"  State-of-the-art extractive question answering models achieve superhuman\nperformances on the SQuAD benchmark. Yet, they are unreasonably heavy and need\nexpensive GPU computing to answer questions in a reasonable time. Thus, they\ncannot be used for real-world queries on hundreds of thousands of documents in\nthe open-domain question answering paradigm. In this paper, we explore the\npossibility to transfer the natural language understanding of language models\ninto dense vectors representing questions and answer candidates, in order to\nmake the task of question-answering compatible with a simple nearest neighbor\nsearch task. This new model, that we call EfficientQA, takes advantage from the\npair of sequences kind of input of BERT-based models to build meaningful dense\nrepresentations of candidate answers. These latter are extracted from the\ncontext in a question-agnostic fashion. Our model achieves state-of-the-art\nresults in Phrase-Indexed Question Answering (PIQA) beating the previous\nstate-of-art by 1.3 points in exact-match and 1.4 points in f1-score. These\nresults show that dense vectors are able to embed very rich semantic\nrepresentations of sequences, although these ones were built from language\nmodels not originally trained for the use-case. Thus, in order to build more\nresource efficient NLP systems in the future, training language models that are\nbetter adapted to build dense representations of phrases is one of the\npossibilities.\n"}
{"id":"2101.02158","authors":"Kenneth L. Clarkson and Sanjana Sahayaraj","title":"Order Embeddings from Merged Ontologies using Sketching","abstract":"  We give a simple, low resource method to produce order embeddings from\nontologies. Such embeddings map words to vectors so that order relations on the\nwords, such as hypernymy\/hyponymy, are represented in a direct way. Our method\nuses sketching techniques, in particular countsketch, for dimensionality\nreduction. We also study methods to merge ontologies, in particular those in\nmedical domains, so that order relations are preserved. We give computational\nresults for medical ontologies and for wordnet, showing that our merging\ntechniques are effective and our embedding yields an accurate representation in\nboth generic and specialised domains.\n"}
{"id":"2101.02235","authors":"Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan\n  Berant","title":"Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit\n  Reasoning Strategies","abstract":"  A key limitation in current datasets for multi-hop reasoning is that the\nrequired steps for answering the question are mentioned in it explicitly. In\nthis work, we introduce StrategyQA, a question answering (QA) benchmark where\nthe required reasoning steps are implicit in the question, and should be\ninferred using a strategy. A fundamental challenge in this setup is how to\nelicit such creative questions from crowdsourcing workers, while covering a\nbroad range of potential strategies. We propose a data collection procedure\nthat combines term-based priming to inspire annotators, careful control over\nthe annotator population, and adversarial filtering for eliminating reasoning\nshortcuts. Moreover, we annotate each question with (1) a decomposition into\nreasoning steps for answering it, and (2) Wikipedia paragraphs that contain the\nanswers to each step. Overall, StrategyQA includes 2,780 examples, each\nconsisting of a strategy question, its decomposition, and evidence paragraphs.\nAnalysis shows that questions in StrategyQA are short, topic-diverse, and cover\na wide range of strategies. Empirically, we show that humans perform well (87%)\non this task, while our best baseline reaches an accuracy of $\\sim$66%.\n"}
{"id":"2101.02244","authors":"Anamaria Crisan, Michael Correll","title":"User Ex Machina : Simulation as a Design Probe in Human-in-the-Loop Text\n  Analytics","abstract":"  Topic models are widely used analysis techniques for clustering documents and\nsurfacing thematic elements of text corpora. These models remain challenging to\noptimize and often require a \"human-in-the-loop\" approach where domain experts\nuse their knowledge to steer and adjust. However, the fragility,\nincompleteness, and opacity of these models means even minor changes could\ninduce large and potentially undesirable changes in resulting model. In this\npaper we conduct a simulation-based analysis of human-centered interactions\nwith topic models, with the objective of measuring the sensitivity of topic\nmodels to common classes of user actions. We find that user interactions have\nimpacts that differ in magnitude but often negatively affect the quality of the\nresulting modelling in a way that can be difficult for the user to evaluate. We\nsuggest the incorporation of sensitivity and \"multiverse\" analyses to topic\nmodel interfaces to surface and overcome these deficiencies.\n"}
{"id":"2101.02258","authors":"Yair Lakretz, Th\\'eo Desbordes, Jean-R\\'emi King, Beno\\^it Crabb\\'e,\n  Maxime Oquab, Stanislas Dehaene","title":"Can RNNs learn Recursive Nested Subject-Verb Agreements?","abstract":"  One of the fundamental principles of contemporary linguistics states that\nlanguage processing requires the ability to extract recursively nested tree\nstructures. However, it remains unclear whether and how this code could be\nimplemented in neural circuits. Recent advances in Recurrent Neural Networks\n(RNNs), which achieve near-human performance in some language tasks, provide a\ncompelling model to address such questions. Here, we present a new framework to\nstudy recursive processing in RNNs, using subject-verb agreement as a probe\ninto the representations of the neural network. We trained six distinct types\nof RNNs on a simplified probabilistic context-free grammar designed to\nindependently manipulate the length of a sentence and the depth of its\nsyntactic tree. All RNNs generalized to subject-verb dependencies longer than\nthose seen during training. However, none systematically generalized to deeper\ntree structures, even those with a structural bias towards learning nested tree\n(i.e., stack-RNNs). In addition, our analyses revealed primacy and recency\neffects in the generalization patterns of LSTM-based models, showing that these\nmodels tend to perform well on the outer- and innermost parts of a\ncenter-embedded tree structure, but poorly on its middle levels. Finally,\nprobing the internal states of the model during the processing of sentences\nwith nested tree structures, we found a complex encoding of grammatical\nagreement information (e.g. grammatical number), in which all the information\nfor multiple words nouns was carried by a single unit. Taken together, these\nresults indicate how neural networks may extract bounded nested tree\nstructures, without learning a systematic recursive rule.\n"}
{"id":"2101.02346","authors":"Yang Li, Amirmohammad Kazameini, Yash Mehta, Erik Cambria","title":"Multitask Learning for Emotion and Personality Detection","abstract":"  In recent years, deep learning-based automated personality trait detection\nhas received a lot of attention, especially now, due to the massive digital\nfootprints of an individual. Moreover, many researchers have demonstrated that\nthere is a strong link between personality traits and emotions. In this paper,\nwe build on the known correlation between personality traits and emotional\nbehaviors, and propose a novel multitask learning framework, SoGMTL that\nsimultaneously predicts both of them. We also empirically evaluate and discuss\ndifferent information-sharing mechanisms between the two tasks. To ensure the\nhigh quality of the learning process, we adopt a MAML-like framework for model\noptimization. Our more computationally efficient CNN-based multitask model\nachieves the state-of-the-art performance across multiple famous personality\nand emotion datasets, even outperforming Language Model based models.\n"}
{"id":"2101.02351","authors":"Ankush Chopra, Shruti Agrawal and Sohom Ghosh","title":"Applying Transfer Learning for Improving Domain-Specific Search\n  Experience Using Query to Question Similarity","abstract":"  Search is one of the most common platforms used to seek information. However,\nusers mostly get overloaded with results whenever they use such a platform to\nresolve their queries. Nowadays, direct answers to queries are being provided\nas a part of the search experience. The question-answer (QA) retrieval process\nplays a significant role in enriching the search experience. Most off-the-shelf\nSemantic Textual Similarity models work fine for well-formed search queries,\nbut their performances degrade when applied to a domain-specific setting having\nincomplete or grammatically ill-formed search queries in prevalence. In this\npaper, we discuss a framework for calculating similarities between a given\ninput query and a set of predefined questions to retrieve the question which\nmatches to it the most. We have used it for the financial domain, but the\nframework is generalized for any domain-specific search engine and can be used\nin other domains as well. We use Siamese network [6] over Long Short-Term\nMemory (LSTM) [3] models to train a classifier which generates unnormalized and\nnormalized similarity scores for a given pair of questions. Moreover, for each\nof these question pairs, we calculate three other similarity scores: cosine\nsimilarity between their average word2vec embeddings [15], cosine similarity\nbetween their sentence embeddings [7] generated using RoBERTa [17] and their\ncustomized fuzzy-match score. Finally, we develop a metaclassifier using\nSupport Vector Machines [19] for combining these five scores to detect if a\ngiven pair of questions is similar. We benchmark our model's performance\nagainst existing State Of The Art (SOTA) models on Quora Question Pairs (QQP)\ndataset as well as a dataset specific to the financial domain.\n"}
{"id":"2101.02352","authors":"Yao Chen, Jiangang Liu, Zhe Zhang, Shiping Wen, Wenjun Xiong","title":"M\\\"{o}biusE: Knowledge Graph Embedding on M\\\"{o}bius Ring","abstract":"  In this work, we propose a novel Knowledge Graph Embedding (KGE) strategy,\ncalled M\\\"{o}biusE, in which the entities and relations are embedded to the\nsurface of a M\\\"{o}bius ring. The proposition of such a strategy is inspired by\nthe classic TorusE, in which the addition of two arbitrary elements is subject\nto a modulus operation. In this sense, TorusE naturally guarantees the critical\nboundedness of embedding vectors in KGE. However, the nonlinear property of\naddition operation on Torus ring is uniquely derived by the modulus operation,\nwhich in some extent restricts the expressiveness of TorusE. As a further\ngeneralization of TorusE, M\\\"{o}biusE also uses modulus operation to preserve\nthe closeness of addition operation on it, but the coordinates on M\\\"{o}bius\nring interacts with each other in the following way: {\\em \\color{red} any\nvector on the surface of a M\\\"{o}bius ring moves along its parametric trace\nwill goes to the right opposite direction after a cycle}. Hence, M\\\"{o}biusE\nassumes much more nonlinear representativeness than that of TorusE, and in turn\nit generates much more precise embedding results. In our experiments,\nM\\\"{o}biusE outperforms TorusE and other classic embedding strategies in\nseveral key indicators.\n"}
{"id":"2101.02359","authors":"Xiangyang Li, Yu Xia, Xiang Long, Zheng Li, Sujian Li","title":"Exploring Text-transformers in AAAI 2021 Shared Task: COVID-19 Fake News\n  Detection in English","abstract":"  In this paper, we describe our system for the AAAI 2021 shared task of\nCOVID-19 Fake News Detection in English, where we achieved the 3rd position\nwith the weighted F1 score of 0.9859 on the test set. Specifically, we proposed\nan ensemble method of different pre-trained language models such as BERT,\nRoberta, Ernie, etc. with various training strategies including\nwarm-up,learning rate schedule and k-fold cross-validation. We also conduct an\nextensive analysis of the samples that are not correctly classified. The code\nis available\nat:https:\/\/github.com\/archersama\/3rd-solution-COVID19-Fake-News-Detection-in-English.\n"}
{"id":"2101.02394","authors":"Yingjie Gu, Xiaoye Qu, Zhefeng Wang, Baoxing Huai, Nicholas Jing Yuan\n  and Xiaolin Gui","title":"Read, Retrospect, Select: An MRC Framework to Short Text Entity Linking","abstract":"  Entity linking (EL) for the rapidly growing short text (e.g. search queries\nand news titles) is critical to industrial applications. Most existing\napproaches relying on adequate context for long text EL are not effective for\nthe concise and sparse short text. In this paper, we propose a novel framework\ncalled Multi-turn Multiple-choice Machine reading comprehension (M3}) to solve\nthe short text EL from a new perspective: a query is generated for each\nambiguous mention exploiting its surrounding context, and an option selection\nmodule is employed to identify the golden entity from candidates using the\nquery. In this way, M3 framework sufficiently interacts limited context with\ncandidate entities during the encoding process, as well as implicitly considers\nthe dissimilarities inside the candidate bunch in the selection stage. In\naddition, we design a two-stage verifier incorporated into M3 to address the\ncommonly existed unlinkable problem in short text. To further consider the\ntopical coherence and interdependence among referred entities, M3 leverages a\nmulti-turn fashion to deal with mentions in a sequence manner by retrospecting\nhistorical cues. Evaluation shows that our M3 framework achieves the\nstate-of-the-art performance on five Chinese and English datasets for the\nreal-world short text EL.\n"}